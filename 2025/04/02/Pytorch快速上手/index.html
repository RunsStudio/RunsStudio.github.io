

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="">
  <meta name="keywords" content="交通">
  
    <meta name="description" content="本笔记从小土堆Pytorch教程中记录一些实用的Pytorch相关操作. 1. 加载数据1.1 PILPIL类可以用于加载图像、保存图像等操作 12from PIL import Imageimg &#x3D; Image.open(&#x27;data&#x2F;hymenoptera_data&#x2F;train&#x2F;ants&#x2F;342438950_a3da61deab.jpg&#x27;) 1.2 DataSetDataSe">
<meta property="og:type" content="article">
<meta property="og:title" content="Pytorch快速入门笔记">
<meta property="og:url" content="https://runsstudio.github.io/2025/04/02/Pytorch%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/index.html">
<meta property="og:site_name" content="Run&#39;s Studio">
<meta property="og:description" content="本笔记从小土堆Pytorch教程中记录一些实用的Pytorch相关操作. 1. 加载数据1.1 PILPIL类可以用于加载图像、保存图像等操作 12from PIL import Imageimg &#x3D; Image.open(&#x27;data&#x2F;hymenoptera_data&#x2F;train&#x2F;ants&#x2F;342438950_a3da61deab.jpg&#x27;) 1.2 DataSetDataSe">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-04-02T12:32:28.000Z">
<meta property="article:modified_time" content="2025-05-08T13:41:08.956Z">
<meta property="article:tag" content="交通">
<meta name="twitter:card" content="summary_large_image">
  
  
  
  <title>Pytorch快速入门笔记 - Run&#39;s Studio</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"runsstudio.github.io","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 65vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Run&#39;s blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/bg/background.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.2)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="Pytorch快速入门笔记"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-04-02 20:32" pubdate>
          2025年4月2日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          4.8k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          40 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">Pytorch快速入门笔记</h1>
            
            
              <div class="markdown-body">
                
                <p>本笔记从<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1hE411t7RN/">小土堆Pytorch教程</a>中记录一些实用的Pytorch相关操作.</p>
<h1 id="1-加载数据"><a href="#1-加载数据" class="headerlink" title="1. 加载数据"></a>1. 加载数据</h1><h2 id="1-1-PIL"><a href="#1-1-PIL" class="headerlink" title="1.1 PIL"></a>1.1 PIL</h2><p>PIL类可以用于加载图像、保存图像等操作</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br>img = Image.<span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;data/hymenoptera_data/train/ants/342438950_a3da61deab.jpg&#x27;</span>)<br></code></pre></td></tr></table></figure>
<h2 id="1-2-DataSet"><a href="#1-2-DataSet" class="headerlink" title="1.2 DataSet"></a>1.2 DataSet</h2><p>DataSet是一个抽象类，需要实现其中的<code>__getitem__</code>方法，以及最好是实现<code>__len__</code>方法，不然不能用迭代器，用for循环的方式取数据,<br>以下是一个自定义数据集的设置方式，可以看到需要重写<code>__getitem__</code>方法取数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyDataSet</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, data_dir, transforms=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.data_dir = data_dir<br>        <span class="hljs-variable language_">self</span>.image_paths = os.listdir(data_dir)<br>        <span class="hljs-variable language_">self</span>.transforms = transforms<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, index</span>) -&gt; T_co:<br>        img_file_name = <span class="hljs-variable language_">self</span>.image_paths[index]<br>        img = Image.<span class="hljs-built_in">open</span>(<span class="hljs-variable language_">self</span>.data_dir + img_file_name)<br>        img_trans = <span class="hljs-variable language_">self</span>.transforms(img)<br>        <span class="hljs-keyword">return</span> img_trans<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.image_paths)<br></code></pre></td></tr></table></figure>

<h2 id="1-3-DataLoader"><a href="#1-3-DataLoader" class="headerlink" title="1.3 DataLoader"></a>1.3 DataLoader</h2><p>如果把DataSet看做一副牌，那么DataLoader就是用于定义如何发牌，或者对牌进行一些操作（洗牌、转换格式等），如果已经有一个数据集，那么可以通过这种方式定义data_loader</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">my_ds = MyDataSet(data_dir=<span class="hljs-string">&#x27;data/hymenoptera_data/val/ants/&#x27;</span>, transforms=trans)<br><span class="hljs-comment"># drop_last: 总长度除bs 除不尽的时候是否去掉最后一个</span><br><span class="hljs-comment"># batch_size: 批的量</span><br>data_loader = DataLoader(dataset=my_ds, batch_size=<span class="hljs-number">2</span>, shuffle=<span class="hljs-literal">True</span>, drop_last=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure>
<p>定义好的数据集，可以通过DataLoader加载，并通过for循环取数据，例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">step = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> images <span class="hljs-keyword">in</span> data_loader:<br>    <span class="hljs-built_in">print</span>(images.shape)<br>    writer.add_images(<span class="hljs-string">&#x27;image_batch&#x27;</span>, images, step)  <span class="hljs-comment"># (tag,Image,step(不添加这个参数 tensorboard里面的step始终为零))</span><br>    step += <span class="hljs-number">1</span><br></code></pre></td></tr></table></figure>
<h2 id="1-4-torchvision数据集的下载和使用"><a href="#1-4-torchvision数据集的下载和使用" class="headerlink" title="1.4 torchvision数据集的下载和使用"></a>1.4 torchvision数据集的下载和使用</h2><p>如果是一些成熟的数据集，比如CIFAR10，可以用封装好的方式获取数据集，这些数据集也是重写了DataSet类，可以传入transform</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">train_data = torchvision.datasets.CIFAR10(root=<span class="hljs-string">&#x27;../data&#x27;</span>, train=<span class="hljs-literal">True</span>, transform=torchvision.transforms.ToTensor(),<br>                                          download=<span class="hljs-literal">True</span>)<br>test_data = torchvision.datasets.CIFAR10(root=<span class="hljs-string">&#x27;../data&#x27;</span>, train=<span class="hljs-literal">False</span>, transform=torchvision.transforms.ToTensor(),<br>                                         download=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure>
<h1 id="2-Tensorboard-的使用"><a href="#2-Tensorboard-的使用" class="headerlink" title="2. Tensorboard 的使用"></a>2. Tensorboard 的使用</h1><p>TensorBoard是一个可视化工具，它可以用来展示网络图、张量的指标变化、张量的分布情况等。特别是在训练网络的时候，我们可以设置不同的参数（比如：权重W、偏置B、卷积层数、全连接层数等），使用TensorBoader可以很直观的帮我们进行参数的选择。它通过运行一个本地服务器，来监听6006端口。在浏览器发出请求时，分析训练时记录的数据，绘制训练过程中的图像。</p>
<p>首先定义一个SummaryWriter()，然后就可以用writer里面的方法往tensorboard里面写数据，不仅可以添加过程量还可以添加单张图像。默认的路径保存到本地runs目录下，可以用SummaryWriter()</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">writer = SummaryWriter()<br><span class="hljs-comment"># 添加过程量（标量）</span><br><span class="hljs-keyword">for</span> n_iter <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    writer.add_scalar(<span class="hljs-string">&#x27;Loss/train&#x27;</span>, np.random.random(), n_iter)<br>    writer.add_scalar(<span class="hljs-string">&#x27;Loss/test&#x27;</span>, np.random.random(), n_iter)<br>    writer.add_scalar(<span class="hljs-string">&#x27;Accuracy/train&#x27;</span>, np.random.random(), n_iter)<br>    writer.add_scalar(<span class="hljs-string">&#x27;Accuracy/test&#x27;</span>, np.random.random(), n_iter)<br>writer.add_image(tag=<span class="hljs-string">&#x27;test&#x27;</span>, img_tensor=img_tensor)<br></code></pre></td></tr></table></figure>
<p>查看数据：cd到保存文件的文件夹下，输入<code>tensorboard --logdir runs</code> runs对应文件保存的目录，然后就可以通过访问<code>http://localhost:6006/#timeseries</code>查看记录的结果</p>
<h1 id="3-Transforms-的使用"><a href="#3-Transforms-的使用" class="headerlink" title="3. Transforms 的使用"></a>3. Transforms 的使用</h1><p>Transforms用来对一张图片进行一系列的转换，可以用Compose定义需要转换的内容</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">trans = transforms.Compose([<br>    transforms.ToTensor(),<br>    <span class="hljs-comment"># transforms.RandomCrop(size=(50,50)) 随机裁剪</span><br>    transforms.Resize((<span class="hljs-number">100</span>, <span class="hljs-number">100</span>))<br>])<br><br></code></pre></td></tr></table></figure>
<p>定义好转换之后，可以对单张图片进行转换，把图像传入就可以，例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 添加图像</span><br>img = Image.<span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;data/hymenoptera_data/train/ants/342438950_a3da61deab.jpg&#x27;</span>)<br><span class="hljs-comment"># 图像转换</span><br>trans = transforms.Compose([<br>    transforms.ToTensor(),<br>    <span class="hljs-comment"># transforms.RandomCrop(size=(50,50)) # 随机裁剪</span><br>])<br>img_tensor = trans(img)<br></code></pre></td></tr></table></figure>

<h1 id="4-常用函数"><a href="#4-常用函数" class="headerlink" title="4. 常用函数"></a>4. 常用函数</h1><h2 id="4-1卷积函数"><a href="#4-1卷积函数" class="headerlink" title="4.1卷积函数"></a>4.1卷积函数</h2><p>卷积函数的定义网上有很多了就不再赘述了，定义一个卷积核，然后和现在的矩阵进行卷积操作，可得到一个结果。</p>
<p>借用知乎<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/161660908">2D卷积,nn.Conv2d和F.conv2d</a>一段话：卷积操作：卷积核和扫过的小区域对应位置相乘再求和的操作，卷积完成后一般要加个偏置bias。一种Kernel如果分成多个通道上的子Kernel做卷积运算，最后运算结果还要加在一起后，再加偏置。</p>
<p>使用卷积运算的时候需要注意输入输出的尺寸，需要对齐，比如Conv2D 如果是函数就要求B ,C 两个维度要对齐。<br>需要注意的点是输入输出维度会根据stride、padding的设置改变，比如64×64的图像进去，不设置padding出来的图像可能就变成62×62了，如果还要保持图像尺寸一致（特别是复现论文的场景），需要反算一下stride和padding的值，这里公式在<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d">Pytorch Conv2d文档</a>，需要的时候直接查阅就好。</p>
<p>关于可视化展示卷积函数中的stride、padding、dilation参数的含义，可参考文档：<a target="_blank" rel="noopener" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">Convolution arithmetic</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br>w = torch.rand(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)  <span class="hljs-comment"># 对应 B C ×卷积核的 H W，前2个维度对应上（在H W两个维度上进行卷积）</span><br>res = F.conv2d(<span class="hljs-built_in">input</span>=a, weight=w, stride=<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(res)<br><br>tensor([[[[-<span class="hljs-number">0.9163</span>, -<span class="hljs-number">1.4657</span>, -<span class="hljs-number">3.6013</span>,  ...,  <span class="hljs-number">0.1913</span>, -<span class="hljs-number">1.4308</span>, -<span class="hljs-number">1.1725</span>],<br>          [-<span class="hljs-number">1.7863</span>, -<span class="hljs-number">1.1487</span>, -<span class="hljs-number">3.5197</span>,  ..., -<span class="hljs-number">0.5010</span>, -<span class="hljs-number">2.3962</span>, -<span class="hljs-number">3.8177</span>],<br>          [-<span class="hljs-number">0.0863</span>, -<span class="hljs-number">0.3723</span>, -<span class="hljs-number">1.7177</span>,  ..., -<span class="hljs-number">1.9196</span>, -<span class="hljs-number">1.4938</span>, -<span class="hljs-number">2.6761</span>],<br>          ...,<br>          [ <span class="hljs-number">0.8136</span>, -<span class="hljs-number">4.5267</span>, -<span class="hljs-number">0.6807</span>,  ..., -<span class="hljs-number">2.2519</span>,  <span class="hljs-number">1.4239</span>, -<span class="hljs-number">0.9793</span>],<br>          [ <span class="hljs-number">1.8353</span>, -<span class="hljs-number">1.8440</span>, -<span class="hljs-number">3.9382</span>,  ..., -<span class="hljs-number">1.8193</span>,  <span class="hljs-number">2.7279</span>,  <span class="hljs-number">4.4726</span>],<br>          [ <span class="hljs-number">0.5444</span>,  <span class="hljs-number">1.2673</span>, -<span class="hljs-number">3.4205</span>,  ..., -<span class="hljs-number">2.3179</span>, -<span class="hljs-number">2.5870</span>, -<span class="hljs-number">1.7544</span>]]]])<br></code></pre></td></tr></table></figure>
<h2 id="4-2-池化函数"><a href="#4-2-池化函数" class="headerlink" title="4.2 池化函数"></a>4.2 池化函数</h2><p>池化函数是深度学习中常用的技术，主要用于降低数据的维度和减少计算量。常见的池化函数包括：</p>
<ol>
<li>最大池化（Max Pooling）：在池化窗口内选取最大值作为输出，能够提取图像中的主要特征。</li>
<li>平均池化（Average Pooling）：在池化窗口内取平均值作为输出，可以平滑输入数据，减少噪声的影响。</li>
<li>自适应池化（Adaptive Pooling）：根据输入的大小自动调整池化窗口的大小，以适应不同的输入尺寸。</li>
</ol>
<p>池化操作可以分为一维池化、二维池化和三维池化，具体取决于被池化的张量维数。池化不仅可以减小数据大小，还可以增加数据大小，具体取决于应用场景。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">a = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">64</span>, <span class="hljs-number">64</span>)<br>res = F.max_pool2d(a,kernel_size=<span class="hljs-number">2</span>) <span class="hljs-comment"># kernel_size指定2的话是默认2×2的2d（正方形）。而且池化默认区域不重叠的，默认步长就是kernel_size=2，这一点和卷积运算不一样</span><br><span class="hljs-built_in">print</span>(res.shape) <span class="hljs-comment"># torch.Size([1, 3, 32, 32])</span><br></code></pre></td></tr></table></figure>

<h1 id="5-神经网络的搭建"><a href="#5-神经网络的搭建" class="headerlink" title="5. 神经网络的搭建"></a>5. 神经网络的搭建</h1><h2 id="5-1-卷积层、池化层、非线性激活层"><a href="#5-1-卷积层、池化层、非线性激活层" class="headerlink" title="5.1 卷积层、池化层、非线性激活层"></a>5.1 卷积层、池化层、非线性激活层</h2><p>通过引入<code>torch.nn</code>引入常见神经网络的层，包括卷积层、池化层等.以及非线性激活层，RELU SOFTMAX之类的，具体就不再展开了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyNeuralNetwork</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, *args, **kwargs</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-built_in">super</span>().__init__(*args, **kwargs)<br>        <span class="hljs-variable language_">self</span>.conv1 = nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">64</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        <span class="hljs-variable language_">self</span>.conv2 = nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">64</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>)<br>        <span class="hljs-variable language_">self</span>.max_pool = nn.MaxPool2d(<span class="hljs-number">4</span>)<br>        <span class="hljs-variable language_">self</span>.relu = nn.ReLU()<br>        <span class="hljs-variable language_">self</span>.softmax = nn.Softmax()<br>        <span class="hljs-comment"># in_channels: int,</span><br>        <span class="hljs-comment"># out_channels: int,</span><br>        <span class="hljs-comment"># kernel_size: _size_2_t,</span><br>        <span class="hljs-comment"># stride: _size_2_t = 1,</span><br>        <span class="hljs-comment"># padding: Union[str, _size_2_t] = 0,</span><br>        <span class="hljs-comment"># dilation: _size_2_t = 1,</span><br><br>a = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">64</span>, <span class="hljs-number">64</span>)<br><br>mnn = MyNeuralNetwork()<br><br>res_conv1 = mnn.conv1(a)<br><span class="hljs-built_in">print</span>(res_conv1.shape)  <span class="hljs-comment"># torch.Size([1, 64, 64, 64])</span><br><br>res_conv2 = mnn.conv2(a)<br><span class="hljs-built_in">print</span>(res_conv2.shape)  <span class="hljs-comment"># torch.Size([1, 64, 62, 62])</span><br><br>res_max_pool = mnn.max_pool(a)<br><span class="hljs-built_in">print</span>(res_max_pool.shape)  <span class="hljs-comment"># torch.Size([1, 3, 16, 16])</span><br><br><br></code></pre></td></tr></table></figure>
<h2 id="5-2线性层及其他层"><a href="#5-2线性层及其他层" class="headerlink" title="5.2线性层及其他层"></a>5.2线性层及其他层</h2><ol>
<li>线性层：线性层又叫全连接层，其中每个神经元和上一层所有的神经元相连，使用<code>nn.Linear(in_features,out_features,bias)</code>定义,运算公式是 $$y&#x3D;xA^T+b$$ ，注意默认是加上bias的，即<code>bias=True</code><br>代码例子：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyNeuralNetwork</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, *args, **kwargs</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-variable language_">self</span>.linear = nn.Linear(<span class="hljs-number">64</span>,<span class="hljs-number">32</span>)<br>a = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">64</span>, <span class="hljs-number">64</span>)<br>res_l = mnn.linear(a)<br><span class="hljs-built_in">print</span>(res_l.shape) <span class="hljs-comment"># torch.Size([1, 3, 64, 32])</span><br></code></pre></td></tr></table></figure>
<ol start="2">
<li>展平层：将多维度的张量展平。默认参数：<code>start_dim: int = 1, end_dim: int = -1</code> 从开始的维度展开到结束的维度<br>代码例子：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyNeuralNetwork</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, *args, **kwargs</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-variable language_">self</span>.flatten1 = nn.Flatten(start_dim=<span class="hljs-number">0</span>)<br>        <span class="hljs-variable language_">self</span>.flatten2 = nn.Flatten()<br>a = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">64</span>, <span class="hljs-number">64</span>)<br>res_f = mnn.flatten1(a)<br>res_f2 = mnn.flatten2(a)<br><span class="hljs-built_in">print</span>(res_f.shape) <span class="hljs-comment"># torch.Size([12288])</span><br><span class="hljs-built_in">print</span>(res_f2.shape) <span class="hljs-comment"># torch.Size([1, 12288])</span><br></code></pre></td></tr></table></figure>
<p>一般说来，Flatten层常用于把多维的输入一维化，常用在从卷积层到全连接层的过渡。Flatten默认不影响batch的大小（start_dim &#x3D;1 ）。</p>
<h2 id="5-3-Sequential的使用"><a href="#5-3-Sequential的使用" class="headerlink" title="5.3 Sequential的使用"></a>5.3 Sequential的使用</h2><p>nn.Sequential() 可以作为容器，里面放入模型的各种层，在forward的时候将会贯序列执行各层，通常有2种定义方式</p>
<ul>
<li>定义方式1</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br>model = nn.Sequential(<br>                  nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">20</span>,<span class="hljs-number">5</span>),<br>                  nn.ReLU(),<br>                  nn.Conv2d(<span class="hljs-number">20</span>,<span class="hljs-number">64</span>,<span class="hljs-number">5</span>),<br>                  nn.ReLU()<br>                )<br> <br><span class="hljs-built_in">print</span>(model)<br><span class="hljs-built_in">print</span>(model[<span class="hljs-number">2</span>]) <span class="hljs-comment"># 通过索引获取第几个层</span><br><span class="hljs-string">&#x27;&#x27;&#x27;运行结果为：</span><br><span class="hljs-string">Sequential(</span><br><span class="hljs-string">  (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))</span><br><span class="hljs-string">  (1): ReLU()</span><br><span class="hljs-string">  (2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))</span><br><span class="hljs-string">  (3): ReLU()</span><br><span class="hljs-string">)</span><br><span class="hljs-string">Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure>
<ul>
<li>定义方式2：给每个层添加一个名称</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> OrderedDict<br>model = nn.Sequential(OrderedDict([<br>                  (<span class="hljs-string">&#x27;conv1&#x27;</span>, nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">20</span>,<span class="hljs-number">5</span>)),<br>                  (<span class="hljs-string">&#x27;relu1&#x27;</span>, nn.ReLU()),<br>                  (<span class="hljs-string">&#x27;conv2&#x27;</span>, nn.Conv2d(<span class="hljs-number">20</span>,<span class="hljs-number">64</span>,<span class="hljs-number">5</span>)),<br>                  (<span class="hljs-string">&#x27;relu2&#x27;</span>, nn.ReLU())<br>                ]))<br> <br><span class="hljs-built_in">print</span>(model)<br><span class="hljs-string">&#x27;&#x27;&#x27;运行结果为：</span><br><span class="hljs-string">Sequential(</span><br><span class="hljs-string">  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))</span><br><span class="hljs-string">  (relu1): ReLU()</span><br><span class="hljs-string">  (conv2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))</span><br><span class="hljs-string">  (relu2): ReLU()</span><br><span class="hljs-string">)</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure>
<p>我们可以将前面所学的层组合起来，形成深层神经网络的架构，例如我们可以编写一个自己的网络如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyNeuralNetwork</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, *args, **kwargs</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-built_in">super</span>().__init__(*args, **kwargs)<br>        <span class="hljs-variable language_">self</span>.model = nn.Sequential(<br>            nn.Conv2d(<span class="hljs-number">3</span>,<span class="hljs-number">64</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),  <span class="hljs-comment"># 【1,64,64,64】 备注：ks=3，stride=1，padding = 1</span><br>            <span class="hljs-comment"># Hout(64) = (Hin(64) + 2×padding - dilation×[ks - 1] × 1 )/stride + 1</span><br>            nn.ReLU(),<br>            nn.Conv2d(<span class="hljs-number">64</span>,<span class="hljs-number">32</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),  <span class="hljs-comment"># 1,32,64,64</span><br>            nn.ReLU(),<br>            nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">16</span>, <span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>), <span class="hljs-comment"># 1,16,64,64</span><br>            nn.ReLU(),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>), <span class="hljs-comment"># 1,16,32,32</span><br>            nn.ReLU(),<br>            nn.Linear(<span class="hljs-number">32</span>,<span class="hljs-number">1024</span>), <span class="hljs-comment"># 1,16,32,1024</span><br>            nn.ReLU(),<br>            nn.Linear(<span class="hljs-number">1024</span>, <span class="hljs-number">1024</span>),  <span class="hljs-comment"># 1,16,32,1024</span><br>            nn.ReLU(),<br>            nn.Flatten(), <span class="hljs-comment"># 1,524288</span><br>            nn.Linear(<span class="hljs-number">524288</span>,<span class="hljs-number">10</span>),  <span class="hljs-comment"># 1,10</span><br>            nn.Softmax(dim= -<span class="hljs-number">1</span>)<br>        )<br><br>mnn = MyNeuralNetwork()<br><br>a = torch.randn(<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">64</span>,<span class="hljs-number">64</span>)<br><br>res = mnn.model(a)<br><br><span class="hljs-built_in">print</span>(res.shape) <span class="hljs-comment"># torch.Size([1, 10])</span><br><span class="hljs-built_in">print</span>(res) <span class="hljs-comment">#tensor([[0.0991, 0.0982, 0.0997, 0.0996, 0.1009, 0.1030, 0.0988, 0.1027, 0.0991,0.0988]], grad_fn=&lt;SoftmaxBackward0&gt;)</span><br></code></pre></td></tr></table></figure>

<p>通过使用Sequential()的方式可以便捷的完成网络的定义，快速实现网络。</p>
<h2 id="5-4-小网络搭建实战"><a href="#5-4-小网络搭建实战" class="headerlink" title="5.4 小网络搭建实战"></a>5.4 小网络搭建实战</h2><p>以vgg16这个网络（图待补充）为例，搭建模型如下（暂未添加Relu层），其实和我们之前写的模型很像</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyNeuralNetwork</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, *args, **kwargs</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-built_in">super</span>().__init__(*args, **kwargs)<br>        <span class="hljs-variable language_">self</span>.model= nn.Sequential(<br>            nn.Conv2d(<span class="hljs-number">3</span>,<span class="hljs-number">32</span>,<span class="hljs-number">5</span>,padding=<span class="hljs-number">2</span>),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br>            nn.Conv2d(<span class="hljs-number">32</span>,<span class="hljs-number">32</span>,<span class="hljs-number">5</span>,padding=<span class="hljs-number">2</span>),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br>            nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, <span class="hljs-number">5</span>, padding=<span class="hljs-number">2</span>),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br>            nn.Flatten(),<br>            nn.Linear(<span class="hljs-number">4096</span>,<span class="hljs-number">64</span>),<br>            nn.Linear(<span class="hljs-number">64</span>, <span class="hljs-number">10</span>),<br>        )<br><br>a = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">64</span>, <span class="hljs-number">64</span>)<br>mnn = MyNeuralNetwork()<br>res = mnn.model(a)<br><span class="hljs-built_in">print</span>(res.shape) <span class="hljs-comment">#torch.Size([1, 10])</span><br></code></pre></td></tr></table></figure>



<h1 id="6-损失函数与反向传播"><a href="#6-损失函数与反向传播" class="headerlink" title="6 损失函数与反向传播"></a>6 损失函数与反向传播</h1><h2 id="6-1-损失函数"><a href="#6-1-损失函数" class="headerlink" title="6.1 损失函数"></a>6.1 损失函数</h2><p>损失函数（Loss Function）是一个衡量预测结果与真实结果之间差异的函数 ，也称为误差函数。它通过计算模型的预测值与真实值之间的不一致程度，来评估模型的性能.<br>根据任务不同，选择的损失函数也不同，对于回归任务，常见的损失函数有<code>MSELoss</code>,对于分类任务常见的损失函数有交叉熵损失<code>CrossEntropyLoss</code><br>交叉熵的损失函数可以描述为 $$loss(x,class) &#x3D; -log(exp(x[class]&#x2F;sum_j(exp(x[j])))&#x3D;-x[class]+ln(sum_j(exp(x[j])]))$$<br>举例说明：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br>x = torch.tensor([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.3</span>]]) <span class="hljs-comment"># 预测三个类别概率分别是0.1,0.2,0.3</span><br>y = torch.tensor([<span class="hljs-number">1</span>]) <span class="hljs-comment"># 答案是1</span><br>loss = F.cross_entropy(x, y) <span class="hljs-comment"># 计算交叉熵 loss = -0.2 + ln(e^0.1+e^0.2+e^0.3) = 1.10194284823</span><br><span class="hljs-built_in">print</span>(loss) <span class="hljs-comment"># tensor(1.1019)</span><br></code></pre></td></tr></table></figure>
<p>其他的案例也差不多</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.tensor([[<span class="hljs-number">0.55</span>, <span class="hljs-number">0.88</span>]]) <span class="hljs-comment"># 预测值</span><br>y = torch.tensor([[<span class="hljs-number">0.5</span>, <span class="hljs-number">0.8</span>]]) <span class="hljs-comment"># 真实值</span><br>loss_l1 = F.l1_loss(x, y) <span class="hljs-comment"># L1Loss 一阶距</span><br>loss_mse = F.mse_loss(x, y) <span class="hljs-comment"># MSE_LOSS</span><br><span class="hljs-built_in">print</span>(loss_l1) <span class="hljs-comment"># tensor(0.0650)</span><br><span class="hljs-built_in">print</span>(loss_mse) <span class="hljs-comment"># tensor(0.0044)</span><br><br>loss_layer = nn.L1Loss(reduction=<span class="hljs-string">&#x27;sum&#x27;</span>)  <span class="hljs-comment"># 备注：reduction默认是mean，用mean的话结果是0.065</span><br>loss_l1_by_layer = loss_layer(x, y)<br><span class="hljs-built_in">print</span>(loss_l1_by_layer)  <span class="hljs-comment"># tensor(0.1300)</span><br></code></pre></td></tr></table></figure>
<h2 id="6-2-反向传播"><a href="#6-2-反向传播" class="headerlink" title="6.2 反向传播"></a>6.2 反向传播</h2><p>首先说一下什么是反向传播算法。<br>反向传播算法(Backpropagation，简称BP算法)是“误差反向传播”的简称，是适合于多层神经元网络的一种学习算法，它建立在梯度下降法的基础上。梯度下降法是训练神经网络的常用方法，许多的训练方法都是基于梯度下降法改良出来的，因此了解梯度下降法很重要。梯度下降法通过计算损失函数的梯度，并将这个梯度反馈给最优化函数来更新权重以最小化损失函数。</p>
<p>在PyTorch中，loss.backward()函数用于计算模型参数相对于损失函数的梯度。</p>
<p>前向传播</p>
<p>首先，模型通过前向传播计算输出值。在这个过程中，PyTorch会记录计算图（Computation Graph），这个计算图记录了从输入到输出的每一步运算及其依赖关系。每个张量（Tensor）都有一个.grad_fn属性，指向一个函数，这个函数描述了如何计算这个张量关于其输入的梯度。</p>
<p>反向传播</p>
<p>当调用loss.backward()时，PyTorch开始反向遍历计算图。这个过程从损失函数开始，沿着图反向传播误差，计算每一个参与运算的张量关于损失的梯度。这是通过链式法则（Chain Rule）完成的，即将损失对某个中间变量的导数分解为其后续操作导数的乘积。</p>
<p>梯度计算<br>在反向传播过程中，每个运算都会计算其输出关于输入的梯度，并将这个梯度累积到输入张量的.grad属性中（如果是标量损失，它没有.grad属性）。这意味着如果一个张量被多个路径使用，它的.grad属性会累积从所有路径来的梯度.</p>
<p>在使用loss.backward()时，有几个重要的注意事项：</p>
<pre><code class="hljs">梯度归零：在每次反向传播之前，通常需要调用optimizer.zero_grad()来将梯度归零，以避免梯度累加
</code></pre>
<h2 id="6-3-优化器"><a href="#6-3-优化器" class="headerlink" title="6.3 优化器"></a>6.3 优化器</h2><p>优化器决定了模型以何种方式的梯度下降算法更新模型。常见的优化器有 SGD, adam等。</p>
<p>在PyTorch中，optimizer.step()是优化器对象的一个方法，用于执行模型参数的更新。在深度学习训练过程中，参数更新是通过反向传播算法计算损失函数的梯度后，使用优化器根据这些梯度进行的。optimizer.step()方法正是用于根据梯度和学习率等超参数来更新模型参数，从而使损失函数值最小化的步骤</p>
<h1 id="7-使用常用模型"><a href="#7-使用常用模型" class="headerlink" title="7 使用常用模型"></a>7 使用常用模型</h1><h2 id="7-1-使用库的方式调用常用模型"><a href="#7-1-使用库的方式调用常用模型" class="headerlink" title="7.1 使用库的方式调用常用模型"></a>7.1 使用库的方式调用常用模型</h2><p>一些常用的模型，比较经典的模型都是包装在库里面了，可以通过<code>torchvision.models.xxx</code>调用模型.<br>例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs python">vgg16 = torchvision.models.vgg16(pretrained = <span class="hljs-literal">False</span>)<br><br><span class="hljs-built_in">print</span>(vgg16)<br><br>输出：<br>VGG(<br>  (features): Sequential(<br>    (<span class="hljs-number">0</span>): Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">64</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>    (<span class="hljs-number">1</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">2</span>): Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>    (<span class="hljs-number">3</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">4</span>): MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">0</span>, dilation=<span class="hljs-number">1</span>, ceil_mode=<span class="hljs-literal">False</span>)<br>    (<span class="hljs-number">5</span>): Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>    (<span class="hljs-number">6</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">7</span>): Conv2d(<span class="hljs-number">128</span>, <span class="hljs-number">128</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>    (<span class="hljs-number">8</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">9</span>): MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">0</span>, dilation=<span class="hljs-number">1</span>, ceil_mode=<span class="hljs-literal">False</span>)<br>    (<span class="hljs-number">10</span>): Conv2d(<span class="hljs-number">128</span>, <span class="hljs-number">256</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>    (<span class="hljs-number">11</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">12</span>): Conv2d(<span class="hljs-number">256</span>, <span class="hljs-number">256</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>    (<span class="hljs-number">13</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">14</span>): Conv2d(<span class="hljs-number">256</span>, <span class="hljs-number">256</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>    (<span class="hljs-number">15</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">16</span>): MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">0</span>, dilation=<span class="hljs-number">1</span>, ceil_mode=<span class="hljs-literal">False</span>)<br>    (<span class="hljs-number">17</span>): Conv2d(<span class="hljs-number">256</span>, <span class="hljs-number">512</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>    (<span class="hljs-number">18</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">19</span>): Conv2d(<span class="hljs-number">512</span>, <span class="hljs-number">512</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>    (<span class="hljs-number">20</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">21</span>): Conv2d(<span class="hljs-number">512</span>, <span class="hljs-number">512</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>    (<span class="hljs-number">22</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">23</span>): MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">0</span>, dilation=<span class="hljs-number">1</span>, ceil_mode=<span class="hljs-literal">False</span>)<br>    (<span class="hljs-number">24</span>): Conv2d(<span class="hljs-number">512</span>, <span class="hljs-number">512</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>    (<span class="hljs-number">25</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">26</span>): Conv2d(<span class="hljs-number">512</span>, <span class="hljs-number">512</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>    (<span class="hljs-number">27</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">28</span>): Conv2d(<span class="hljs-number">512</span>, <span class="hljs-number">512</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>    (<span class="hljs-number">29</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">30</span>): MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">0</span>, dilation=<span class="hljs-number">1</span>, ceil_mode=<span class="hljs-literal">False</span>)<br>  )<br>  (avgpool): AdaptiveAvgPool2d(output_size=(<span class="hljs-number">7</span>, <span class="hljs-number">7</span>))<br>  (classifier): Sequential(<br>    (<span class="hljs-number">0</span>): Linear(in_features=<span class="hljs-number">25088</span>, out_features=<span class="hljs-number">4096</span>, bias=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">1</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">2</span>): Dropout(p=<span class="hljs-number">0.5</span>, inplace=<span class="hljs-literal">False</span>)<br>    (<span class="hljs-number">3</span>): Linear(in_features=<span class="hljs-number">4096</span>, out_features=<span class="hljs-number">4096</span>, bias=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">4</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">5</span>): Dropout(p=<span class="hljs-number">0.5</span>, inplace=<span class="hljs-literal">False</span>)<br>    (<span class="hljs-number">6</span>): Linear(in_features=<span class="hljs-number">4096</span>, out_features=<span class="hljs-number">1000</span>, bias=<span class="hljs-literal">True</span>)<br>  )<br>)<br><br></code></pre></td></tr></table></figure>

<h2 id="7-2-对常用模型进行增加或修改"><a href="#7-2-对常用模型进行增加或修改" class="headerlink" title="7.2 对常用模型进行增加或修改"></a>7.2 对常用模型进行增加或修改</h2><ol>
<li>增加某层</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">vgg16.features.add_module(<span class="hljs-string">&#x27;relu&#x27;</span>,torch.nn.ReLU())<br>vgg16.classifier.add_module(<span class="hljs-string">&#x27;linear&#x27;</span>,torch.nn.Linear(<span class="hljs-number">1000</span>,<span class="hljs-number">10</span>))<br>例如分类器加上之后，模型结构如下：<br>  (classifier): Sequential(<br>    (<span class="hljs-number">0</span>): Linear(in_features=<span class="hljs-number">25088</span>, out_features=<span class="hljs-number">4096</span>, bias=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">1</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">2</span>): Dropout(p=<span class="hljs-number">0.5</span>, inplace=<span class="hljs-literal">False</span>)<br>    (<span class="hljs-number">3</span>): Linear(in_features=<span class="hljs-number">4096</span>, out_features=<span class="hljs-number">4096</span>, bias=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">4</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">5</span>): Dropout(p=<span class="hljs-number">0.5</span>, inplace=<span class="hljs-literal">False</span>)<br>    (<span class="hljs-number">6</span>): Linear(in_features=<span class="hljs-number">4096</span>, out_features=<span class="hljs-number">1000</span>, bias=<span class="hljs-literal">True</span>)<br>    (linear): Linear(in_features=<span class="hljs-number">1000</span>, out_features=<span class="hljs-number">10</span>, bias=<span class="hljs-literal">True</span>)<br>  )<br></code></pre></td></tr></table></figure>

<ol start="2">
<li>修改某层</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">vgg16.classifier[<span class="hljs-number">7</span>]=nn.Linear(<span class="hljs-number">1000</span>,<span class="hljs-number">20</span>) <span class="hljs-comment"># 7 是模型的第几层</span><br><br>上面的代码执行之后会对刚添加的线性层修改输出特征节点的个数，改成了<span class="hljs-number">55</span>个<br>  (classifier): Sequential(<br>    (<span class="hljs-number">0</span>): Linear(in_features=<span class="hljs-number">25088</span>, out_features=<span class="hljs-number">4096</span>, bias=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">1</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">2</span>): Dropout(p=<span class="hljs-number">0.5</span>, inplace=<span class="hljs-literal">False</span>)<br>    (<span class="hljs-number">3</span>): Linear(in_features=<span class="hljs-number">4096</span>, out_features=<span class="hljs-number">4096</span>, bias=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">4</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">5</span>): Dropout(p=<span class="hljs-number">0.5</span>, inplace=<span class="hljs-literal">False</span>)<br>    (<span class="hljs-number">6</span>): Linear(in_features=<span class="hljs-number">4096</span>, out_features=<span class="hljs-number">1000</span>, bias=<span class="hljs-literal">True</span>)<br>    (linear): Linear(in_features=<span class="hljs-number">1000</span>, out_features=<span class="hljs-number">55</span>, bias=<span class="hljs-literal">True</span>)<br>  )<br></code></pre></td></tr></table></figure>

<ol start="3">
<li>删除某层<br>如果想删除某一层,直接将其删除即可，命令为</li>
</ol>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-selector-tag">del</span> vgg16<span class="hljs-selector-class">.classifier</span><span class="hljs-selector-attr">[7]</span><br></code></pre></td></tr></table></figure>

<ol start="4">
<li>冻结部分层<br>我们现在只想训练最后的fc1层，然后就有了下面的</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 冻结fc1层的参数</span><br><span class="hljs-keyword">for</span> name, param <span class="hljs-keyword">in</span> model.named_parameters():<br>    <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;fc1&quot;</span> <span class="hljs-keyword">in</span> name:<br>        param.requires_grad = <span class="hljs-literal">False</span><br><br><span class="hljs-comment"># 只传入需要更新的参数给优化器</span><br>optimizer = optim.SGD(<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> p: p.requires_grad, model.parameters()), lr=<span class="hljs-number">1e-2</span>)<br></code></pre></td></tr></table></figure>

<h1 id="8-完整的训练流程"><a href="#8-完整的训练流程" class="headerlink" title="8 完整的训练流程"></a>8 完整的训练流程</h1><p>包括数据集准备，dataLoader准备、网络构建、损失函数定义、循环、计算误差、tensorboard可视化等</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br><br>train_data = torchvision.datasets.CIFAR10(root=<span class="hljs-string">&#x27;/data&#x27;</span>, train=<span class="hljs-literal">True</span>, transform=torchvision.transforms.ToTensor(),<br>                                          download=<span class="hljs-literal">True</span>)<br>test_data = torchvision.datasets.CIFAR10(root=<span class="hljs-string">&#x27;/data&#x27;</span>, train=<span class="hljs-literal">False</span>, transform=torchvision.transforms.ToTensor(),<br>                                         download=<span class="hljs-literal">True</span>)<br><br>train_data_size = <span class="hljs-built_in">len</span>(train_data)<br>test_data_size = <span class="hljs-built_in">len</span>(test_data)<br><br>train_data_loader = DataLoader(train_data, batch_size=<span class="hljs-number">64</span>)<br>test_data_loader = DataLoader(test_data, batch_size=<span class="hljs-number">64</span>)<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyNeuralNetwork</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, *args, **kwargs</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-built_in">super</span>().__init__(*args, **kwargs)<br>        <span class="hljs-variable language_">self</span>.model = nn.Sequential(<br>            nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br>            nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br>            nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br>            nn.Flatten(),<br>            nn.Linear(<span class="hljs-number">64</span> * <span class="hljs-number">4</span> * <span class="hljs-number">4</span>, <span class="hljs-number">64</span>),<br>            nn.Linear(<span class="hljs-number">64</span>, <span class="hljs-number">10</span>)<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.model(x)<br><br><br>mnn = MyNeuralNetwork()<br><span class="hljs-keyword">if</span> torch.cuda.is_available():<br>    mnn = mnn.cuda()<br><br>loss_fn = nn.CrossEntropyLoss()<br>loss_fn = loss_fn.cuda()<br><br>lr = <span class="hljs-number">1e-2</span><br>optim = torch.optim.SGD(mnn.parameters(), lr=lr)<br><br>writer = SummaryWriter(<span class="hljs-string">&#x27;../logs_train&#x27;</span>)<br>epoch = <span class="hljs-number">10</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epoch):<br>    mnn.train()<br>    train_step = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> train_data_loader:<br>        imgs, targets = data<br>        imgs = imgs.cuda()<br>        targets = targets.cuda()<br>        outputs = mnn(imgs)<br>        loss = loss_fn(outputs, targets)<br><br>        optim.zero_grad()<br>        loss.backward()<br>        optim.step()<br><br>        train_step = train_step + <span class="hljs-number">1</span><br>        <span class="hljs-keyword">if</span> train_step % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;训练次数：&#123;&#125;，loss = &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(train_step, loss.item()))<br>            writer.add_scalar(<span class="hljs-string">&quot;train_loss&quot;</span>, loss.item(), train_step)<br><br>    mnn.<span class="hljs-built_in">eval</span>()<br>    total_test_loss = <span class="hljs-number">0</span><br>    total_accuracy = <span class="hljs-number">0</span><br>    total_test_step = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br><br>        <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> test_data_loader:<br>            imgs, targets = data<br>            imgs = imgs.cuda()<br>            targets = targets.cuda()<br>            outputs = mnn(imgs)<br>            loss = loss_fn(outputs, targets)<br>            total_test_loss += loss.item()<br>            <span class="hljs-comment"># 求正确率</span><br>            accuracy = (outputs.argmax(<span class="hljs-number">1</span>) == targets).<span class="hljs-built_in">sum</span>()<br>            total_accuracy += accuracy<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;整体测试集上的loss:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(total_test_loss))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;整体测试集上的正确率:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(total_accuracy / test_data_size))<br>    writer.add_scalar(<span class="hljs-string">&quot;test_loss&quot;</span>, total_test_loss, total_test_step)<br>    writer.add_scalar(<span class="hljs-string">&quot;test_accuracy&quot;</span>, total_accuracy / test_data_size, total_test_step)<br>    total_test_step = total_test_step + <span class="hljs-number">1</span><br><br>    <span class="hljs-comment"># 保存模型</span><br>    torch.save(mnn, <span class="hljs-string">&quot;mnn&#123;&#125;.pth&quot;</span>.<span class="hljs-built_in">format</span>(i))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;模型已保存&quot;</span>)<br><br>writer.close()<br><br></code></pre></td></tr></table></figure>
<p>运行代码的效果：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs text">训练次数：100，loss = 2.2861804962158203<br>训练次数：200，loss = 2.2651848793029785<br>训练次数：300，loss = 2.2193050384521484<br>训练次数：400，loss = 2.1087183952331543<br>训练次数：500，loss = 2.0523011684417725<br>训练次数：600，loss = 1.9955447912216187<br>训练次数：700，loss = 1.9990053176879883<br>整体测试集上的loss:319.1352970600128<br>整体测试集上的正确率:0.2669000029563904<br>模型已保存<br></code></pre></td></tr></table></figure>
<h1 id="8-1-使用GPU训练"><a href="#8-1-使用GPU训练" class="headerlink" title="8.1 使用GPU训练"></a>8.1 使用GPU训练</h1><p>除了8.1的方式在所有张量用<code>.cuda()</code>送入显存的方式使用GPU训练外，还可以用<code>tensor.to(device)</code>的方式送入显存</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">device = <span class="hljs-string">&#x27;cuda&#x27;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;cpu&#x27;</span><br><br>net = Net().to(device)<br>imgs = imgs.to(device)<br></code></pre></td></tr></table></figure>
                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="category-chain-item">深度学习</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>Pytorch快速入门笔记</div>
      <div>https://runsstudio.github.io/2025/04/02/Pytorch快速上手/</div>
    </div>
    <div class="license-meta">
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年4月2日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/04/23/%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0/" title="多模态表征学习 - 学习笔记">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">多模态表征学习 - 学习笔记</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/03/31/hello-world/" title="Run&#39;s Studio 重新出发">
                        <span class="hidden-mobile">Run&#39;s Studio 重新出发</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
