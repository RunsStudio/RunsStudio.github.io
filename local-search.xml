<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Pytorch快速入门笔记</title>
    <link href="/2025/04/02/Pytorch%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/"/>
    <url>/2025/04/02/Pytorch%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/</url>
    
    <content type="html"><![CDATA[<p>从<a href="https://www.bilibili.com/video/BV1hE411t7RN/">小土堆Pytorch教程</a>中记录一些实用的Pytorch相关操作.</p><h1 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h1><h2 id="PIL"><a href="#PIL" class="headerlink" title="PIL"></a>PIL</h2><p>PIL类可以用于加载图像、保存图像等操作</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br>img = Image.<span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;data/hymenoptera_data/train/ants/342438950_a3da61deab.jpg&#x27;</span>)<br></code></pre></td></tr></table></figure><h2 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h2><p>DataSet是一个抽象类，需要实现其中的<code>__getitem__</code>方法，以及最好是实现<code>__len__</code>方法，不然不能用迭代器，用for循环的方式取数据,<br>以下是一个自定义数据集的设置方式，可以看到需要重写<code>__getitem__</code>方法取数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyDataSet</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, data_dir, transforms=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.data_dir = data_dir<br>        <span class="hljs-variable language_">self</span>.image_paths = os.listdir(data_dir)<br>        <span class="hljs-variable language_">self</span>.transforms = transforms<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, index</span>) -&gt; T_co:<br>        img_file_name = <span class="hljs-variable language_">self</span>.image_paths[index]<br>        img = Image.<span class="hljs-built_in">open</span>(<span class="hljs-variable language_">self</span>.data_dir + img_file_name)<br>        img_trans = <span class="hljs-variable language_">self</span>.transforms(img)<br>        <span class="hljs-keyword">return</span> img_trans<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.image_paths)<br></code></pre></td></tr></table></figure><h2 id="DataLoader"><a href="#DataLoader" class="headerlink" title="DataLoader"></a>DataLoader</h2><p>如果把DataSet看做一副牌，那么DataLoader就是用于定义如何发牌，或者对牌进行一些操作（洗牌、转换格式等），如果已经有一个数据集，那么可以通过这种方式定义data_loader</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">my_ds = MyDataSet(data_dir=<span class="hljs-string">&#x27;data/hymenoptera_data/val/ants/&#x27;</span>, transforms=trans)<br><span class="hljs-comment"># drop_last: 总长度除bs 除不尽的时候是否去掉最后一个</span><br><span class="hljs-comment"># batch_size: 批的量</span><br>data_loader = DataLoader(dataset=my_ds, batch_size=<span class="hljs-number">2</span>, shuffle=<span class="hljs-literal">True</span>, drop_last=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p>定义好的数据集，可以通过DataLoader加载，并通过for循环取数据，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">step = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> images <span class="hljs-keyword">in</span> data_loader:<br>    <span class="hljs-built_in">print</span>(images.shape)<br>    writer.add_images(<span class="hljs-string">&#x27;image_batch&#x27;</span>, images, step)  <span class="hljs-comment"># (tag,Image,step(不添加这个参数 tensorboard里面的step始终为零))</span><br>    step += <span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><h2 id="torchvision数据集的下载和使用"><a href="#torchvision数据集的下载和使用" class="headerlink" title="torchvision数据集的下载和使用"></a>torchvision数据集的下载和使用</h2><p>如果是一些成熟的数据集，比如CIFAR10，可以用封装好的方式获取数据集，这些数据集也是重写了DataSet类，可以传入transform</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">train_data = torchvision.datasets.CIFAR10(root=<span class="hljs-string">&#x27;../data&#x27;</span>, train=<span class="hljs-literal">True</span>, transform=torchvision.transforms.ToTensor(),<br>                                          download=<span class="hljs-literal">True</span>)<br>test_data = torchvision.datasets.CIFAR10(root=<span class="hljs-string">&#x27;../data&#x27;</span>, train=<span class="hljs-literal">False</span>, transform=torchvision.transforms.ToTensor(),<br>                                         download=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><h1 id="Tensorboard-的使用"><a href="#Tensorboard-的使用" class="headerlink" title="Tensorboard 的使用"></a>Tensorboard 的使用</h1><p>首先定义一个SummaryWriter()，然后就可以用writer里面的方法往tensorboard里面写数据，不仅可以添加过程量还可以添加单张图像。默认的路径保存到本地runs目录下，可以用SummaryWriter()</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">writer = SummaryWriter()<br><span class="hljs-comment"># 添加过程量（标量）</span><br><span class="hljs-keyword">for</span> n_iter <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    writer.add_scalar(<span class="hljs-string">&#x27;Loss/train&#x27;</span>, np.random.random(), n_iter)<br>    writer.add_scalar(<span class="hljs-string">&#x27;Loss/test&#x27;</span>, np.random.random(), n_iter)<br>    writer.add_scalar(<span class="hljs-string">&#x27;Accuracy/train&#x27;</span>, np.random.random(), n_iter)<br>    writer.add_scalar(<span class="hljs-string">&#x27;Accuracy/test&#x27;</span>, np.random.random(), n_iter)<br>writer.add_image(tag=<span class="hljs-string">&#x27;test&#x27;</span>, img_tensor=img_tensor)<br></code></pre></td></tr></table></figure><p>查看数据：cd到保存文件的文件夹下，输入<code>tensorboard --logdir runs</code> runs对应文件保存的目录，然后就可以通过访问<code>http://localhost:6006/#timeseries</code>查看记录的结果</p><h1 id="Transforms-的使用"><a href="#Transforms-的使用" class="headerlink" title="Transforms 的使用"></a>Transforms 的使用</h1><p>Transforms用来对一张图片进行一系列的转换，可以用Compose定义需要转换的内容</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">trans = transforms.Compose([<br>    transforms.ToTensor(),<br>    <span class="hljs-comment"># transforms.RandomCrop(size=(50,50)) 随机裁剪</span><br>    transforms.Resize((<span class="hljs-number">100</span>, <span class="hljs-number">100</span>))<br>])<br><br></code></pre></td></tr></table></figure><p>定义好转换之后，可以对单张图片进行转换，把图像传入就可以，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 添加图像</span><br>img = Image.<span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;data/hymenoptera_data/train/ants/342438950_a3da61deab.jpg&#x27;</span>)<br><span class="hljs-comment"># 图像转换</span><br>trans = transforms.Compose([<br>    transforms.ToTensor(),<br>    <span class="hljs-comment"># transforms.RandomCrop(size=(50,50)) # 随机裁剪</span><br>])<br>img_tensor = trans(img)<br></code></pre></td></tr></table></figure><h1 id="常用函数"><a href="#常用函数" class="headerlink" title="常用函数"></a>常用函数</h1><h2 id="卷积函数"><a href="#卷积函数" class="headerlink" title="卷积函数"></a>卷积函数</h2><p>卷积函数的定义网上有很多了就不再赘述了，定义一个卷积核，然后和现在的矩阵进行卷积操作，可得到一个结果。</p><p>借用知乎<a href="https://zhuanlan.zhihu.com/p/161660908">2D卷积,nn.Conv2d和F.conv2d</a>一段话：卷积操作：卷积核和扫过的小区域对应位置相乘再求和的操作，卷积完成后一般要加个偏置bias。一种Kernel如果分成多个通道上的子Kernel做卷积运算，最后运算结果还要加在一起后，再加偏置。</p><p>使用卷积运算的时候需要注意输入输出的尺寸，需要对齐，比如Conv2D 如果是函数就要求B ,C 两个维度要对齐。<br>需要注意的点是输入输出维度会根据stride、padding的设置改变，比如64×64的图像进去，不设置padding出来的图像可能就变成62×62了，如果还要保持图像尺寸一致（特别是复现论文的场景），需要反算一下stride和padding的值，这里公式在<a href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d">Pytorch Conv2d文档</a>，需要的时候直接查阅就好。</p><p>关于可视化展示卷积函数中的stride、padding、dilation参数的含义，可参考文档：<a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">Convolution arithmetic</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br>w = torch.rand(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)  <span class="hljs-comment"># 对应 B C ×卷积核的 H W，前2个维度对应上（在H W两个维度上进行卷积）</span><br>res = F.conv2d(<span class="hljs-built_in">input</span>=a, weight=w, stride=<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(res)<br><br>tensor([[[[-<span class="hljs-number">0.9163</span>, -<span class="hljs-number">1.4657</span>, -<span class="hljs-number">3.6013</span>,  ...,  <span class="hljs-number">0.1913</span>, -<span class="hljs-number">1.4308</span>, -<span class="hljs-number">1.1725</span>],<br>          [-<span class="hljs-number">1.7863</span>, -<span class="hljs-number">1.1487</span>, -<span class="hljs-number">3.5197</span>,  ..., -<span class="hljs-number">0.5010</span>, -<span class="hljs-number">2.3962</span>, -<span class="hljs-number">3.8177</span>],<br>          [-<span class="hljs-number">0.0863</span>, -<span class="hljs-number">0.3723</span>, -<span class="hljs-number">1.7177</span>,  ..., -<span class="hljs-number">1.9196</span>, -<span class="hljs-number">1.4938</span>, -<span class="hljs-number">2.6761</span>],<br>          ...,<br>          [ <span class="hljs-number">0.8136</span>, -<span class="hljs-number">4.5267</span>, -<span class="hljs-number">0.6807</span>,  ..., -<span class="hljs-number">2.2519</span>,  <span class="hljs-number">1.4239</span>, -<span class="hljs-number">0.9793</span>],<br>          [ <span class="hljs-number">1.8353</span>, -<span class="hljs-number">1.8440</span>, -<span class="hljs-number">3.9382</span>,  ..., -<span class="hljs-number">1.8193</span>,  <span class="hljs-number">2.7279</span>,  <span class="hljs-number">4.4726</span>],<br>          [ <span class="hljs-number">0.5444</span>,  <span class="hljs-number">1.2673</span>, -<span class="hljs-number">3.4205</span>,  ..., -<span class="hljs-number">2.3179</span>, -<span class="hljs-number">2.5870</span>, -<span class="hljs-number">1.7544</span>]]]])<br></code></pre></td></tr></table></figure><h2 id="池化函数"><a href="#池化函数" class="headerlink" title="池化函数"></a>池化函数</h2><p>池化函数是深度学习中常用的技术，主要用于降低数据的维度和减少计算量。常见的池化函数包括：</p><ol><li>最大池化（Max Pooling）：在池化窗口内选取最大值作为输出，能够提取图像中的主要特征。</li><li>平均池化（Average Pooling）：在池化窗口内取平均值作为输出，可以平滑输入数据，减少噪声的影响。</li><li>自适应池化（Adaptive Pooling）：根据输入的大小自动调整池化窗口的大小，以适应不同的输入尺寸。</li></ol><p>池化操作可以分为一维池化、二维池化和三维池化，具体取决于被池化的张量维数。池化不仅可以减小数据大小，还可以增加数据大小，具体取决于应用场景。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">a = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">64</span>, <span class="hljs-number">64</span>)<br>res = F.max_pool2d(a,kernel_size=<span class="hljs-number">2</span>) <span class="hljs-comment"># kernel_size指定2的话是默认2×2的2d（正方形）。而且池化默认区域不重叠的，默认步长就是kernel_size=2，这一点和卷积运算不一样</span><br><span class="hljs-built_in">print</span>(res.shape) <span class="hljs-comment"># torch.Size([1, 3, 32, 32])</span><br></code></pre></td></tr></table></figure><h1 id="神经网络的搭建"><a href="#神经网络的搭建" class="headerlink" title="神经网络的搭建"></a>神经网络的搭建</h1><h2 id="卷积层、池化层、非线性激活层"><a href="#卷积层、池化层、非线性激活层" class="headerlink" title="卷积层、池化层、非线性激活层"></a>卷积层、池化层、非线性激活层</h2><p>通过引入<code>torch.nn</code>引入常见神经网络的层，包括卷积层、池化层等.以及非线性激活层，RELU SOFTMAX之类的，具体就不再展开了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyNeuralNetwork</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, *args, **kwargs</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-built_in">super</span>().__init__(*args, **kwargs)<br>        <span class="hljs-variable language_">self</span>.conv1 = nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">64</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        <span class="hljs-variable language_">self</span>.conv2 = nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">64</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>)<br>        <span class="hljs-variable language_">self</span>.max_pool = nn.MaxPool2d(<span class="hljs-number">4</span>)<br>        <span class="hljs-variable language_">self</span>.relu = nn.ReLU()<br>        <span class="hljs-variable language_">self</span>.softmax = nn.Softmax()<br>        <span class="hljs-comment"># in_channels: int,</span><br>        <span class="hljs-comment"># out_channels: int,</span><br>        <span class="hljs-comment"># kernel_size: _size_2_t,</span><br>        <span class="hljs-comment"># stride: _size_2_t = 1,</span><br>        <span class="hljs-comment"># padding: Union[str, _size_2_t] = 0,</span><br>        <span class="hljs-comment"># dilation: _size_2_t = 1,</span><br><br>a = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">64</span>, <span class="hljs-number">64</span>)<br><br>mnn = MyNeuralNetwork()<br><br>res_conv1 = mnn.conv1(a)<br><span class="hljs-built_in">print</span>(res_conv1.shape)  <span class="hljs-comment"># torch.Size([1, 64, 64, 64])</span><br><br>res_conv2 = mnn.conv2(a)<br><span class="hljs-built_in">print</span>(res_conv2.shape)  <span class="hljs-comment"># torch.Size([1, 64, 62, 62])</span><br><br>res_max_pool = mnn.max_pool(a)<br><span class="hljs-built_in">print</span>(res_max_pool.shape)  <span class="hljs-comment"># torch.Size([1, 3, 16, 16])</span><br><br><br></code></pre></td></tr></table></figure><h2 id="线性层及其他层"><a href="#线性层及其他层" class="headerlink" title="线性层及其他层"></a>线性层及其他层</h2><p>未完待续</p><h2 id="Sequential的使用"><a href="#Sequential的使用" class="headerlink" title="Sequential的使用"></a>Sequential的使用</h2>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Run&#39;s Studio 重新出发</title>
    <link href="/2025/03/31/hello-world/"/>
    <url>/2025/03/31/hello-world/</url>
    
    <content type="html"><![CDATA[<pre><code class="hljs">原先网站的source因为换电脑的缘故没有保存，只能重新开一份博客，记录工作中的心得体会~</code></pre>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>安装和配置Pytorch和cuda</title>
    <link href="/2025/03/16/%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AEPytorch%E5%92%8Ccuda/"/>
    <url>/2025/03/16/%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AEPytorch%E5%92%8Ccuda/</url>
    
    <content type="html"><![CDATA[<h1 id="安装方法"><a href="#安装方法" class="headerlink" title="安装方法"></a>安装方法</h1><p>请参考：<a href="https://www.cnblogs.com/tryhardwy/p/14659131.html">https://www.cnblogs.com/tryhardwy/p/14659131.html</a></p><ol><li>卸载掉旧版本torch torchvision</li><li>先到（<a href="https://pytorch.org/get-started/locally/%EF%BC%89%E6%9F%A5%E5%88%B0%E7%A8%B3%E5%AE%9A%E7%89%88%E6%9C%ACtorch%E5%AF%B9%E5%BA%94%E7%9A%84cuda">https://pytorch.org/get-started/locally/）查到稳定版本torch对应的cuda</a></li><li>下载并安装cuda</li><li>到（<a href="https://pytorch.org/get-started/locally/%EF%BC%89%E6%8C%89%E7%85%A7%E5%AF%B9%E5%BA%94%E7%89%88%E6%9C%AC%E5%AE%89%E8%A3%85torch">https://pytorch.org/get-started/locally/）按照对应版本安装torch</a></li><li>安装完成后，进入python验证。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br>torch.__version__<br>torch.cuda.is_available()<br></code></pre></td></tr></table></figure><p>显示True则安装成功。</p><h1 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h1><p>注意：直接粘贴</p><p><code>pip install torch torchvision torchaudio -f https://download.pytorch.org/whl/torch_stable.html</code> ，安装的是cpu版！</p><p>带cuda版本正确的安装语句是：</p><p><code>pip3 install torch==2.6.0+cu118 torchvision==0.21.0+cu118 --index-url https://download.pytorch.org/whl/cu118</code></p><p>如果上述地址下载太慢，还可以换用国内源例如aliyun（备注：阿里云的torch版本不全，部分最新版本无法下载，经过实验2.1.0版本可以下载并安装）<br><code>pip install torch==2.1.0+cu118 --use-deprecated=legacy-resolver  --no-cache-dir -f https://mirrors.aliyun.com/pytorch-wheels/cu118</code></p><p>其他的包 如果国内地址下载太慢，可以用以下常用镜像：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs awk">常见镜像源<br><br>清华：https:<span class="hljs-regexp">//</span>pypi.tuna.tsinghua.edu.cn<span class="hljs-regexp">/simple/</span><br>阿里云：http:<span class="hljs-regexp">//mi</span>rrors.aliyun.com<span class="hljs-regexp">/pypi/</span>simple/<br>中国科技大学：https:<span class="hljs-regexp">//</span>pypi.mirrors.ustc.edu.cn<span class="hljs-regexp">/simple/</span><br>华中科技大学：http:<span class="hljs-regexp">//</span>pypi.hustunique.com<span class="hljs-regexp">/simple/</span><br>上海交通大学：https:<span class="hljs-regexp">//mi</span>rror.sjtu.edu.cn<span class="hljs-regexp">/pypi/</span>web<span class="hljs-regexp">/simple/</span><br></code></pre></td></tr></table></figure><p>如果使用不带https的连接，还需要加上<code>--trusted-host mirrors.xxx.com</code><br>例如<br><code>pip install numpy&lt;=2.0.0 -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com</code></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Hexo新建博客并上传Github全流程</title>
    <link href="/2025/03/04/hexo%E6%96%B0%E5%BB%BA%E6%93%8D%E4%BD%9C%E5%B9%B6%E4%B8%8A%E4%BC%A0%E6%B5%81%E7%A8%8B/"/>
    <url>/2025/03/04/hexo%E6%96%B0%E5%BB%BA%E6%93%8D%E4%BD%9C%E5%B9%B6%E4%B8%8A%E4%BC%A0%E6%B5%81%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="step-1-安装node-js和git环境"><a href="#step-1-安装node-js和git环境" class="headerlink" title="step 1. 安装node.js和git环境"></a>step 1. 安装node.js和git环境</h1><p>可参考网上教程</p><h1 id="step-2-安装hexo"><a href="#step-2-安装hexo" class="headerlink" title="step 2. 安装hexo"></a>step 2. 安装hexo</h1><p>此时已经安装好了node.js和git，下面开始hexo的安装<br>首先在某一磁盘目录下创建文件夹，例如F盘，创建文件夹名为 Blog<br>进入Blog文件夹，右键鼠标-&gt;选择<code>Git Bash Here</code><br>输入 <code>npm install -g hexo-cli</code> ，并耐心等待一段时间<br>输入 <code>npm install hexo -save</code>，也耐心等待一段时间<br>此时我们可以发现，在Blog文件夹中多了许多内容<br>在Blog文件夹中新建文件夹，命名为hexo<br>关闭当前Git Bash，进入hexo文件夹，右键鼠标-&gt;选择Git Bash Here，或者直接在当前的Git Bash中输入 cd hexo<br>输入hexo init，初始化hexo环境，耐心等待一段时间<br>输入npm install，安装npm依赖包，耐心等待一段时间<br>输入hexo generate或者是hexo g，生成静态页面，耐心等待一段时间<br>输入hexo server或者是hexo s，生成本地服务。我们每次写完博客后，可以先在本地预览一下看看有没有什么问题，然后再发布到网上。<br>接着，我们在浏览器中访问<a href="http://localhost:4000/%EF%BC%8C%E8%BF%99%E5%B0%B1%E6%98%AF%E5%9C%A8%E6%9C%AC%E5%9C%B0%E7%94%9F%E6%88%90%E7%9A%84%E4%B8%80%E4%B8%AA%E5%8D%9A%E5%AE%A2%E3%80%82">http://localhost:4000/，这就是在本地生成的一个博客。</a><br>在Git Bash中按下Ctrl+C，可以关闭当前端口服务。</p><p>到这里，我们的本地博客就已经搭建完成了。下面将介绍如何与Github连接，将博客上传Internet</p><h1 id="step-3-新建Github仓库"><a href="#step-3-新建Github仓库" class="headerlink" title="step 3. 新建Github仓库"></a>step 3. 新建Github仓库</h1><p>登录到自己的Github中，新建一个仓库，命名为username.github.io，其中的username是你的用户名，勾选Initialiaze this repository with a README，创建仓库<br>我们可以访问自己的<code>username.github.io</code><br>返回<code>username.github.io</code>的仓库中，复制Git地址</p><h1 id="step-4-本地操作"><a href="#step-4-本地操作" class="headerlink" title="step 4.本地操作"></a>step 4.本地操作</h1><p>我们在<code>/Blog/hexo/</code>文件夹中，找到<code>_config.yml</code>文件，用文本编辑器打开它<br>将最下面的deploy改为下图所示的内容，其中repo的地址就是刚才我们复制的Git地址，修改好后保存退出 【注】修改内容中的:和后面的字母之间要有一个空格，否则后续内容会报错<br>接下来，我们暂且不考虑新建文章，在<code>Git Bash</code>中执行<code>npm install hexo-deployer-git </code>–save命令，耐心等待一段时间<br>最后执行 <code>hexo deploy</code>或者<code>hexo d</code>【注】这一步需要保证Github上拥有本机的公钥，可以自行查找解决办法<br>最后，成功部署</p><h1 id="step-5-上传博客"><a href="#step-5-上传博客" class="headerlink" title="step 5.上传博客"></a>step 5.上传博客</h1><h2 id="新建文章"><a href="#新建文章" class="headerlink" title="新建文章"></a>新建文章</h2><p>在Git Bash中输入<code>hexo new title</code>，其中，title就是我们这篇文章的名字。我们可以看到，在\Blog\hexo\source_posts\ 文件夹中新建了一个名称为<code>Test1.md</code>的文件<br>我们去编辑一下这个文件，此处需要Linux的部分知识，可以自行上网查找<br>编辑结束后，保存退出</p><h2 id="上传"><a href="#上传" class="headerlink" title="上传"></a>上传</h2><p>使用hexo g，生成静态文件<br>使用hexo d来将文档部署到Github上<br>最后我们访问username.github.io，发现刚才编辑的文档已经成功发布到了Internet上面<br>————————————————</p><p>版权声明：本文为博主原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接和本声明。</p><p>原文链接：<a href="https://blog.csdn.net/qq_43669381/article/details/107823432">https://blog.csdn.net/qq_43669381/article/details/107823432</a></p><p>部署流程（带图版）：<a href="https://blog.csdn.net/clearloe/article/details/139879493">https://blog.csdn.net/clearloe/article/details/139879493</a></p><h2 id="乱码问题解决方法"><a href="#乱码问题解决方法" class="headerlink" title="乱码问题解决方法"></a>乱码问题解决方法</h2><p>需要注意：如果本地部署没有出现乱码，但是主题上传到github上可能出现乱码，排版不正常的情况，此时要修改 <code>__config.yaml</code> 文件,修改：<br>url: <a href="https://yourpage.github.io/">https://yourpage.github.io</a> （自己的主页的网址，最后不要有&#x2F;）<br>root: &#x2F;   （增加这一行）<br>保存后重新生成并部署即可正常显示。</p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
