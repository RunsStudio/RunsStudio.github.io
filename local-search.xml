<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>TransformerLight - 学习笔记</title>
    <link href="/2025/05/28/TransformerLight%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <url>/2025/05/28/TransformerLight%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="TransformerLight-学习笔记"><a href="#TransformerLight-学习笔记" class="headerlink" title="TransformerLight - 学习笔记"></a>TransformerLight - 学习笔记</h1><h2 id="1-文章摘要"><a href="#1-文章摘要" class="headerlink" title="1. 文章摘要"></a>1. 文章摘要</h2><p>交通信号控制 （TSC） 仍然是交通领域最重要和最具挑战性的研究问题之一。在线强化学习 （RL） 在 TSC 中取得了巨大成功，但由于过多的试错学习过程，在实际应用中的学习成本非常高。离线 RL 是一种很有前途的降低学习成本的方法，然而，离线强化学习主要面对数据分布偏移问题，这些问题仍然悬而未决。为此，在本文中，我们将交通控制表述为一个序列建模问题，其中包含由交通环境中的<strong>状态、动作和奖励</strong>描述的马尔可夫决策过程序列。从而引入了一种新的框架，即 TransformerLight，它的目的不是通过平均所有可能的回报来拟合值函数（过往的模型常用方法，神经网络用于拟合RL中的V值或Q值），而是使用门控 Transformer 产生最佳动作。此外，TransformerLight 的学习过程通过用动态的引起的门控Transformer（Gated Transformer）块替换残差连接而更加稳定。通过对离线数据集的数值实验，证明了TransformerLight模型（1）无需动态规划即可构建高性能自适应的信号控制模型（2）与BCQ、CQL等离线强化学习模型相比展现了更好的性能（3）相比传统的离线强化学习，性能更加稳定。</p><p>论文：<a href="https://dl.acm.org/doi/10.1145/3580305.3599530">https://dl.acm.org/doi/10.1145/3580305.3599530</a></p><p>代码：<a href="https://github.com/Smart-Trafficlab/TransformerLight">https://github.com/Smart-Trafficlab/TransformerLight</a></p><h2 id="2-模型结构"><a href="#2-模型结构" class="headerlink" title="2. 模型结构"></a>2. 模型结构</h2>]]></content>
    
    
    <categories>
      
      <category>信号控制</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>一种简单且实用的单点自适应信号控制模型：OPAC</title>
    <link href="/2025/05/28/%E4%B8%80%E7%A7%8D%E7%AE%80%E5%8D%95%E4%B8%94%E5%AE%9E%E7%94%A8%E7%9A%84%E4%BF%A1%E5%8F%B7%E6%8E%A7%E5%88%B6%E6%A8%A1%E5%9E%8B%E2%80%94%E2%80%94OPAC/"/>
    <url>/2025/05/28/%E4%B8%80%E7%A7%8D%E7%AE%80%E5%8D%95%E4%B8%94%E5%AE%9E%E7%94%A8%E7%9A%84%E4%BF%A1%E5%8F%B7%E6%8E%A7%E5%88%B6%E6%A8%A1%E5%9E%8B%E2%80%94%E2%80%94OPAC/</url>
    
    <content type="html"><![CDATA[<h1 id="一种简单且实用的单点自适应信号控制模型：OPAC"><a href="#一种简单且实用的单点自适应信号控制模型：OPAC" class="headerlink" title="一种简单且实用的单点自适应信号控制模型：OPAC"></a>一种简单且实用的单点自适应信号控制模型：OPAC</h1>]]></content>
    
    
    <categories>
      
      <category>信号控制</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Decision Transformer - 学习笔记</title>
    <link href="/2025/05/28/Decision%20Transformer%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <url>/2025/05/28/Decision%20Transformer%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="Decision-Transformer-学习笔记"><a href="#Decision-Transformer-学习笔记" class="headerlink" title="Decision Transformer 学习笔记"></a>Decision Transformer 学习笔记</h1><h2 id="1-文章摘要"><a href="#1-文章摘要" class="headerlink" title="1. 文章摘要"></a>1. 文章摘要</h2><p>Transformer模型是一种seq2seq的模型，它的独特之处在于，给定一个输入序列，由模型自己决定输出序列的内容与长度。最开始很自然地应用在NLP类似的序列问题的解决上，不过很多问题都可以建模为seq2seq问题，从而这个模型也可以解决许许多多不同的问题，如：语音识别，词性标注，图像物体识别等。该文章将Transformer尝试使用在强化学习领域，提出一种将强化学习建模为序列决策任务的框架，称为Decision Tranformer 。模型的思想很简单：基于期望回报（Return To Go）、历史状态以及历史动作输出下一刻的最优动作。与传统的拟合值函数或者计算策略梯度的强化学习方法不同，Decision Tranformer是一种“离线强化学习（Offline Reinforcement Learning）”模型，更具体的说就是在模型训练的过程中<strong>完全不与真实环境进行交互</strong>，通过监督学习的方式，从历史样本中学习专家经验。</p><p>模型通过利用casually masked Transformer来输出最优动作。通过自回归（autoregressive 意思就是把自己当前和之前的所有输出作为下一次的输入，迭代产生一个输出序列）的方式运行，历史序列+当前的序列不断运行，让本文的Transformer模型可以生成相应的动作去达成期望回报。</p><p>论文：<a href="https://arxiv.org/abs/2106.01345">https://arxiv.org/abs/2106.01345</a></p><p>代码：<a href="https://github.com/kzl/decision-transformer">https://github.com/kzl/decision-transformer</a></p><h2 id="2-模型结构"><a href="#2-模型结构" class="headerlink" title="2. 模型结构"></a>2. 模型结构</h2><h3 id="2-1-整体模型结构"><a href="#2-1-整体模型结构" class="headerlink" title="2.1 整体模型结构"></a>2.1 整体模型结构</h3><p><img src="/2025/05/28/Decision%20Transformer%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image.png" alt="Decision Transformer 模型结构"></p><p>DT的模型结构如图所示，状态、动作、回报被投入各自对应的embedding中，并进行位置编码。这里位置编码采用的是时间戳编码。R,S,A组成的token被送入GPT结构中，以自回归的方式，结合因果掩码（causal mask）预测下一个时刻的动作。</p><h3 id="2-2-模型分类"><a href="#2-2-模型分类" class="headerlink" title="2.2 模型分类"></a>2.2 模型分类</h3><p>由于dt中没有model（model的作用是用于预测未来的状态），故模型属于<strong>model-free</strong>模型。<br>由于dt是采用离线数据进行训练，训练过程中不与真实环境进行交互，故模型属于<strong>offline RL</strong>模型。</p><h3 id="2-3-回报设计"><a href="#2-3-回报设计" class="headerlink" title="2.3 回报设计"></a>2.3 回报设计</h3><p>与传统的强化学习不同，文章希望transformer从历史序列中学习到动作的信息，并用于<strong>预测未来</strong>的动作。然而，对奖励函数进行建模又是不现实的，文章因此使用了reward-to-go（RTG）作为轨迹在训练过程中的reward，而非reward的原始值。</p><p>在测试的时候，只需要给定一个<strong>期望的奖励</strong>，以及初始状态即可。在实际环境中运行，得到实际奖励之后，就将期望奖励减去这个实际奖励，再迭代送入input。</p><h2 id="3-模型伪代码"><a href="#3-模型伪代码" class="headerlink" title="3. 模型伪代码"></a>3. 模型伪代码</h2><p><img src="/2025/05/28/Decision%20Transformer%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-1.png" alt="伪代码"></p><p>模型伪代码如图所示，基本上和上图结构一致。首先R S A送入各自的embedding，然后进行stack操作（类似concat，可以理解把三张表按照合并列的方式拼接）。随后送入transformer模型中，得到隐状态（hidden_state），并根据隐状态进行动作选择，最终得到预测动作并执行。</p><p>在评价回合执行动作后，获取剩余奖励，减去RTG，和S A拼接成token送入模型继续预测下一个动作。</p><h2 id="4-实验部分"><a href="#4-实验部分" class="headerlink" title="4. 实验部分"></a>4. 实验部分</h2><p>（待补充）</p><h2 id="5-参考文献"><a href="#5-参考文献" class="headerlink" title="5. 参考文献"></a>5. 参考文献</h2><p><a href="https://zhuanlan.zhihu.com/p/501117104">https://zhuanlan.zhihu.com/p/501117104</a></p>]]></content>
    
    
    <categories>
      
      <category>强化学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>STD-MAE - 学习笔记</title>
    <link href="/2025/05/22/STD-MAE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <url>/2025/05/22/STD-MAE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="STD-MAE-学习笔记"><a href="#STD-MAE-学习笔记" class="headerlink" title="STD-MAE - 学习笔记"></a>STD-MAE - 学习笔记</h1><h2 id="1-文章摘要"><a href="#1-文章摘要" class="headerlink" title="1. 文章摘要"></a>1. 文章摘要</h2><p>与典型的多变量时间序列相比，时空数据和时序数据的关键区别在于时空异质性。简单来说就是不同地点（市中心和郊区）和日期（比如工作日和周末）的时间序列规律可能有所不同，但他们在相似环境中表现出一致且可预测的模式。因此，准确预测时空数据的关键在于有效捕捉这种时空异质性。之前的研究者们提出了许多关于时空预测的尝试，比如把图卷积网络（GCN）嵌入到时序卷积网络（TCN）或递归神经网络（RNN）中，或者沿着时空轴应用Transformer架构模型。然而这些模型在区分时空异质性方面存在困难。如何学习清晰的时空异质性仍然是时空预测的主要挑战。<br>基于此，Gao等人提出一种新的时空解耦掩码预训练框架（STD-MAE）。它为学习清晰且完整的时空异质性提供高效且有效解决方案，且预训练与下游任务解耦，使得学习到的时空异质性可以无缝集成到下游任务中。</p><p><img src="/2025/05/22/STD-MAE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1280X1280.PNG" alt="时空异质性"></p><p>文章举例说明：不同检测器在时空规律上相差很大，导致同一时间点，不同流量检测的时间序列相近的情况下，未来流量的变化趋势差距可能非常大，导致预测不准。文章把这种现象称为“时空幻觉”。</p><p>论文原文：<a href="https://arxiv.org/abs/2312.00516">https://arxiv.org/abs/2312.00516</a></p><p>代码地址：<a href="https://github.com/Jimmy-7664/STD-MAE">https://github.com/Jimmy-7664/STD-MAE</a></p><h2 id="2-模型结构"><a href="#2-模型结构" class="headerlink" title="2. 模型结构"></a>2. 模型结构</h2><p><img src="/2025/05/22/STD-MAE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1280X1280-1.PNG" alt="模型结构"></p><p>时空解耦掩码自编码器由时间自编码器（T-MAE）和空间自编码器（S-MAE）组成，他们具有相似的结构。S-MAE在空间维度上应用自注意力机制，而T-MAE则在时间维度上应用自注意力机制。<br>从模型结构上看，时间Encoder和空间Encoder的作用是分别对时空状态进行表征。所以整个模型的训练分成两阶段：预训练阶段和下游任务的训练阶段。</p><h2 id="2-1-预训练阶段"><a href="#2-1-预训练阶段" class="headerlink" title="2.1 预训练阶段"></a>2.1 预训练阶段</h2><p>在预训练阶段以以掩码重构的方式进行预训练，此时模型的结构是Encoder-Decoder，采用自监督的方式进行训练：掩码一定比例的序列并尝试还原。预训练结束后，只使用Encoder即可表征时空。Decoder不再使用。<br>预训练阶段，首先对预测的输入进行分块，采用分块的方式进行输入。每条长序列分割成Tp个Patch。其中</p><p><img src="/2025/05/22/STD-MAE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1280X1280-1-1.PNG"></p><p>每个patch包含L条数据，每个数据里面又有C个channel，所以时空Encoder的输入尺寸是 </p><p><img src="/2025/05/22/STD-MAE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1280X1280-2.PNG"></p><p>随后，输入Xp经过全连接层，进行维度嵌入，嵌入后维度变成</p><p><img src="/2025/05/22/STD-MAE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1280X1280-1-2.PNG"></p><p>随后对于嵌入后的矩阵，进行二维时空位置编码。传统的Transformer是位置编码，是一维的编码，本文面向有时空两个维度，因此要进行时空二维编码，掩码的生成方式依然是使用正弦和余弦函数进行修改。</p><p><img src="/2025/05/22/STD-MAE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1280X1280-3.PNG" alt="时空二维编码"></p><p>获得位置编码矩阵Epos后，和嵌入矩阵Ep相加，得到最终输入嵌入E：</p><p><img src="/2025/05/22/STD-MAE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1280X1280-1-3.PNG"></p><p>输入嵌入E随后分别进行时空掩码，获得 可见的掩码嵌入。其中，时间维度上是对时间进行掩码，假设掩码比例是r，那么时间掩码（T-MAE）后的张量尺寸就是Tp(1-r)×N×D，空间掩码（S-MAE）后的张量尺寸就是Tp×N（1-r）×D。<br>文章对掩码的比例r进行了敏感性分析，虽然在图像上，更高的掩码比例可以获得更好的效果，但是在时空领域，25%的掩码率效果是最好的。</p><p><img src="/2025/05/22/STD-MAE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1280X1280-4.PNG" alt="掩码超参实验"></p><p>掩码后的矩阵随后输入到Transformer Layers， 输出的隐向量即为时序和空间的状态表达。但是只得到状态表达空间是没法训练的，还需要通过下游decoder恢复原始的序列。那么整个的过程和Encoder是相反的：首先，对于掩码掉的部分进行恢复，填充Padding，填充后对填充的部分进行位置编码，随后输出被掩码掉部分的序列，尝试对掩码的序列进行还原。并使用还原后掩码部分的序列与原序列掩码的部分进行Loss计算，可表示为：</p><p><img src="/2025/05/22/STD-MAE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E6%97%A0%E6%A0%87%E9%A2%98.png" alt="损失函数"></p><h2 id="2-2-下游任务训练阶段"><a href="#2-2-下游任务训练阶段" class="headerlink" title="2.2 下游任务训练阶段"></a>2.2 下游任务训练阶段</h2><p>STD-MAE可以无缝与现有的预测框架整合。该操作是添加时空状态表达，把STD-MAE训练好的Representation直接添加到预测器的隐藏状态层中。</p><p>比如，预测期经过某层，可以得到包含隐信息的张量Hf，这里Hf相当于是经过下游预测器的表征层之后的输出。</p><p><img src="/2025/05/22/STD-MAE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E6%97%A0%E6%A0%87%E9%A2%98-1.png"></p><p>随后，对下游任务的表征层应用ST-Encoder，具体应用方法很简单：把得到的ST-Encoder在时间维度上截取最后的一个时间戳，形成N×T’×D的张量，T’&#x3D;1，然后经过全连接层和原状态表征直接加起来，得到新的状态表征，即：</p><p><img src="/2025/05/22/STD-MAE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E6%97%A0%E6%A0%87%E9%A2%98-2.png"></p><p>对应模型结构图中彩色方块。最终，经过FC层，输出最终的预测结果Y。</p><p>文章指出，下游任务可以是接标准预测任务的PipeLine，可以等效替换任意的模型。文章害测试了DCRNN、MTGNN、STID、STAE、GWNet作为下游的预测期，没改下游的代码，发现GWNet是效果最好的。<br>这里有一个细节，就是时空编码器和原始状态表征 接入的位置，看起来是接入在模型的中间层，偏后的位置，相当于原模型也对数据进行了一部分表征。</p><h1 id="3-实验部分"><a href="#3-实验部分" class="headerlink" title="3 实验部分"></a>3 实验部分</h1><p>模型在多个数据集上和多个历史模型进行了测试，评测指标MAE、 RMSE、MAPE，用过去的12个时间步预测未来的12个时间步，取得了SOTA的效果。</p><p><img src="/2025/05/22/STD-MAE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E6%97%A0%E6%A0%87%E9%A2%98-1-1.png"></p><p>MASK的消融实验部分，对比了只使用时间维度Mask、空间维度Mask，时空融合MAE，以及不用MASK，最终结果显示是时空解耦的方式的MAE效果最好</p><p><img src="/2025/05/22/STD-MAE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E6%97%A0%E6%A0%87%E9%A2%98-3.png" alt="消融实验"></p><p>对于不同的预测器，只要是用了STD-MAE进行增强，模型的指标性能都有所提升，GWNet增强效果最好。</p><p><img src="/2025/05/22/STD-MAE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E6%97%A0%E6%A0%87%E9%A2%98-1-2.png" alt="不同预测器的增强结果"></p>]]></content>
    
    
    <categories>
      
      <category>时空图</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Graph WaveNet - 学习笔记</title>
    <link href="/2025/05/06/Graph%20WaveNet%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <url>/2025/05/06/Graph%20WaveNet%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="Graph-WaveNet-学习笔记"><a href="#Graph-WaveNet-学习笔记" class="headerlink" title="Graph WaveNet - 学习笔记"></a>Graph WaveNet - 学习笔记</h1><h2 id="1-文章摘要"><a href="#1-文章摘要" class="headerlink" title="1. 文章摘要"></a>1. 文章摘要</h2><p>时空图建模是分析时间关系和空间关系的重要任务，过去的方法大多数都是从固定的图结构上提取空间依赖性，假设实体之间的基本关系是预先确定的。但是，显式的图结构不一定反应真实的以来关系，而且由于数据中连接不完整，可能缺少真正的关系。RNN和CNN又有各自的缺陷，无法捕捉长时间的序列，因此，该文章提出一种新的图神经网络架构Graph WaveNet用于时空图建模，通过开发一种新的自适应依赖关系矩阵，并通过节点嵌入的方式来进行学习。该模型可以捕捉数据中隐藏的空间以来关系，借助堆叠的膨胀1-D卷积，能够随着层数增加感受到宽广的感受野，从而处理非常长的序列。<br>该论文主要用于解决时空建模问题上图结构不确定性问题，通过自适应的可学习的邻接矩阵从数据中自动学习图结构，该论文是基于wavenet网络改进的。<br>论文主要思路：<br>时空图建模背后的一个基本假设是：一个节点的未来信息取决于它的历史信息以及它邻居节点的历史信息。但是这种模型存在两个主要的缺点：</p><ul><li><p>显式的图结构不能充分的反应真实的依赖关系（空间）：</p><ul><li>连接不需要两个节点之间的相互依赖关系</li><li>两个节点之间的相互依赖关系存在但连接缺失</li></ul></li><li><p>时空图不能有效地学习时间依赖关系（时间）：</p><ul><li>基于RNN的方法在捕获长序列时存在耗时的迭代传播和梯度爆炸&#x2F;消失现象；</li><li>基于CNN的方法具有并行计算、稳定梯度和低内存需求等优点。 然而，由于采用标准的一维卷积，其感受野大小随隐藏层数的增加而线性增长，因此需要使用许多层才能捕获很长的序列。</li></ul></li></ul><p>论文原文：<a href="https://arxiv.org/pdf/1906.00121.pdf">https://arxiv.org/pdf/1906.00121.pdf</a></p><p>代码地址：github.com&#x2F;nnzhan&#x2F;Graph-WaveNet</p><h2 id="2-问题定义"><a href="#2-问题定义" class="headerlink" title="2. 问题定义"></a>2. 问题定义</h2><p>交通预测问题，可以认为是给定一张图</p><p><img src="/2025/05/06/Graph%20WaveNet%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image.png" alt="图结构"></p><p>其中V是节点，E是边，交通预测问题可以描述为：<br><img src="/2025/05/06/Graph%20WaveNet%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-1.png" alt="交通预测问题"><br>式中：<br><img src="/2025/05/06/Graph%20WaveNet%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-2.png"><br>分别表示X个特征在过去T个时刻的值（流量变化情况），这里N是节点数，D是数据维数，T是时间步，简单来说就是用过去的S步预测未来的T步。</p><h2 id="3-空间卷积"><a href="#3-空间卷积" class="headerlink" title="3. 空间卷积"></a>3. 空间卷积</h2><p>GCN时代：<br><img src="/2025/05/06/Graph%20WaveNet%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-3.png" alt="GCN公式"><br>其中要求邻接矩阵A已知，实际上很多情况下，A可能是变化的，或者存在未能被挖掘到的节点，对当前节点存在影响。文章不用传统的GCN，而是用了扩散的卷积层，形式如下：<br><img src="/2025/05/06/Graph%20WaveNet%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-4.png" alt="扩散图卷积公式"><br>式中：Pk代表是转移矩阵的k次乘方，K的次数是可以改变的，X是原来的特征，对于无向图，P&#x3D;A&#x2F;rowsum(A)，对于有向图，区分正反向，正向是Pf&#x3D;A&#x2F;rowsum（A），反向是Pb&#x3D;At &#x2F;rowsum(A_T)<br>从而扩散图卷积层可以写成式4的形式。</p><h2 id="4-自适应邻接矩阵"><a href="#4-自适应邻接矩阵" class="headerlink" title="4. 自适应邻接矩阵"></a>4. 自适应邻接矩阵</h2><p>文章同时提出一种自适应邻接矩阵的概念，这种矩阵不需要任何先验知识，而且是可以从端到端的方式进行梯度下降训练。可以表示成：<br><img src="/2025/05/06/Graph%20WaveNet%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-5.png" alt="自适应邻接矩阵定义"><br>这里面，E1 E2是两个可学习的Embedding矩阵，案例来说应该就是原始输入X乘了一个Embedding矩阵之后得到的。这个公式的形式，和自注意力的计算公式不能说一模一样只能说完全一致<br><img src="/2025/05/06/Graph%20WaveNet%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-6.png" alt="自注意力的计算公式"><br>（自注意力的计算公式，Q Kt分别对应这里的E1 E2）区别在于文章这里加了个Relu （这是GAT的做法）只关注对当前节点正向的内容。<br>所以，在图已知的情况下，GWNet可以用式6的方式计算 图卷积，如果是图未知的情况下，就用公式7计算<br><img src="/2025/05/06/Graph%20WaveNet%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-7.png" alt="GVNet提出的图卷积计算公式"><br>文章提到：值得注意的是，我们的图卷积属于基于空间的方法。尽管为了保持一致性，我们将图信号与节点特征矩阵互换使用，但我们在方程 7 中的图卷积确实被解释为聚合来自不同邻域顺序的转换特征信息。</p><h2 id="5-时间卷积层"><a href="#5-时间卷积层" class="headerlink" title="5. 时间卷积层"></a>5. 时间卷积层</h2><p>时间卷积文章采用了 空洞因果卷积 作为时间卷积层（TCN），空洞卷积神经网络能够以非递归的方式处理长距离序列。公式可以描述成：</p><p><img src="/2025/05/06/Graph%20WaveNet%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-8.png" alt="时序卷积公式"></p><p><img src="/2025/05/06/Graph%20WaveNet%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-11.png" alt="时序卷积示意图"></p><h2 id="6-门控TCN："><a href="#6-门控TCN：" class="headerlink" title="6. 门控TCN："></a>6. 门控TCN：</h2><p>门控机制在RNN中特别重要，可以有效的控制信息的流动（在层和层之间流动），在TCN中也是这样。该文章采用的门控TCN采用如下方式：</p><p>g()是激活函数，σ（·）是sigmoid函数，决定了信息传递到下一层的比例</p><h2 id="7-参考文献："><a href="#7-参考文献：" class="headerlink" title="7. 参考文献："></a>7. 参考文献：</h2><p><a href="http://zhuanlan.zhihu.com/p/594429261">http://zhuanlan.zhihu.com/p/594429261</a></p>]]></content>
    
    
    <categories>
      
      <category>时空图</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>STIDGCN - 学习笔记</title>
    <link href="/2025/04/28/STIDGCN%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <url>/2025/04/28/STIDGCN%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="STIDGCN-学习笔记"><a href="#STIDGCN-学习笔记" class="headerlink" title="STIDGCN - 学习笔记"></a>STIDGCN - 学习笔记</h1><h2 id="1-文章摘要"><a href="#1-文章摘要" class="headerlink" title="1. 文章摘要"></a>1. 文章摘要</h2><p>准确的交通预测对于城市交通管理、路线规划和流量检测至关重要。时空模型的最新进展显着改进了交通预测中复杂的时空相关性的建模。不幸的是，之前的大多数研究在跨不同感知视角有效建模时空相关性方面遇到了挑战，并且忽略了时空相关性之间的交互学习。此外，受空间异质性的限制，大多数研究未能考虑每个节点不同的时空模式。为了克服这些限制，我们提出了一种用于流量预测的时空交互式动态图卷积网络（STIDGCN）。具体来说，文章提出了一个由空间和时间模块组成的交互式学习框架，用于对流量数据进行下采样。该框架旨在通过采用从全局到局部的感知视角来捕捉空间和时间的相关性，并通过积极的反馈促进它们的相互利用。在空间模块中，我们基于图构造方法设计了动态图卷积网络。该网络旨在利用考虑时空异质性的流量模式库作为查询来重建数据驱动的动态图结构。重构的图结构可以揭示交通网络中节点之间的动态关联。对八个真实世界流量数据集的大量实验表明，STIDGCN  在平衡计算成本的同时优于最先进的基线。</p><p>源代码：<br><a href="https://github.com/LiuAoyu1998/STIDGCN/blob/main/model.py">源代码</a></p><p>论文：<br><a href="https://ieeexplore.ieee.org/abstract/document/10440184">论文</a></p><h2 id="2-模型核心结构"><a href="#2-模型核心结构" class="headerlink" title="2. 模型核心结构"></a>2. 模型核心结构</h2><p><img src="/2025/04/28/STIDGCN%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image0.png" alt="核心结构"></p><ul><li>STI结构把输入分成多个序列，并且向下游不断分裂，形成类似二叉树的结构。这样做的目的：类似时序卷积，例如原始序列id：12345，分裂后序列A就是135，B就是246，这样序列A就能侧重分析到索引1和3之间的关系，如果原始数据是五分钟级别的，做一次STI可以认为变成关注十分钟级别的，两次就是关注二十分钟级别的。</li><li>创新点：创新了一个DGCN模块，把原始序列分成奇序列（<img src="/2025/04/28/STIDGCN%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image.png" alt="奇数序列">）和偶序列<img src="/2025/04/28/STIDGCN%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-1.png" alt="偶数序列">，送入时序卷积块中提取卷积信息，然后送到DGCN块中提取空间信息。最重要的一点是是<strong>交互学习</strong>，也就是看图中的红线，奇序列经过图模块提取完空间信息后的隐向量将会与偶序列进行哈德马积（红线、绿线），反过来偶数序列时序卷积、DGCN后也和奇序列进行哈德玛积。这个过程会进行两次，因此形成一个时空交互的结构，这个过程是本文最大的创新点。</li></ul><h2 id="3-各模块介绍"><a href="#3-各模块介绍" class="headerlink" title="3. 各模块介绍"></a>3. 各模块介绍</h2><h3 id="3-1-Encoder"><a href="#3-1-Encoder" class="headerlink" title="3.1 Encoder"></a>3.1 Encoder</h3><p><strong>STI模块</strong><br>STI模块里面包含时间卷积TSConv模块和图卷积DGCN模块，两个模块在STI模块中进行交互学习。</p><p><strong>TSConv模块</strong><br>文章提出使用TSConv模块作为时间模块，捕捉时间关联性。<br>时序卷积模块使用二维的CNN对padding后的序列进行卷积，两层卷积分别使用（stride &#x3D; 1，padding &#x3D; s1,）、（1，s2）作为卷积核，s1、s2是预先定义好的核尺寸，文章后续对该参数进行了敏感性分析。TSConv可与定义为如下公式：<br><img src="/2025/04/28/STIDGCN%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-2.png" alt="卷积公式"></p><p>其中H_t 和 H_t’ 代表了TSConv的隐状态，这里省略了激活函数。<br>通过两层的卷积，能够提取到单个序列上的时序的动态性</p><p><strong>DGCN模块</strong><br>文章设计了一个权重共享的DGCN模块，作为空间模块，捕捉空间动态相关性。<br>因为文章处理的是动态图，预先不知道图的邻接矩阵，得通过DGCN结构提取空间状态表征<br>分成两个步骤：<br>①动态图重构：模拟动态的邻接矩阵<br>②对于构建的动态图，聚合周边节点的信息<br>输入是TSConv模块学习后的嵌入表示，这里输入的维度是Hg∈R（C×N×t’），C表示隐藏层的维度，channel，N是节点的个数，t’是时序的长度。<br>这里由于输入的 尺寸不一样，因此先经过一个全连接层，得到聚合输入Hf ∈R（C×N），随后和模式库（φ，Pattern Bank）进行交叉注意力（Hf和φ，得到Ap）和自注意力（Hf和Hf，得到Ah）计算。这里Pattern Bank是一个可训练的矩阵，φ∈R（C×N），可以认为是一种节点嵌入。得到嵌入矩阵后，进行交叉注意力【公式（6）】和自注意力【公式（7）】计算。：<br> <img src="/2025/04/28/STIDGCN%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-3.png" alt="注意力计算公式"><br>如此操作后，会得到两个邻接矩阵，Ap和Ah，这是两个矩阵，大小都是N×N<br>然后把得到的两个空间注意力邻接矩阵使用Concat操作拼接，得到一个2N×N的矩阵，为了和下游的尺度对齐，又经过一个全连接层，这一步的目的是把矩阵的尺度从N×2N变成N×N<br> <img src="/2025/04/28/STIDGCN%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-4.png" alt="全连接层公式"><br>经过全连接层输出Af矩阵，可以认为Af矩阵中包含了节点和节点之间的空间关联关系，这是一个N×N的动态邻接矩阵。用这种方式计算的邻接矩阵会计算所有节点的空间关联性，然而实际上在图中不是所有节点都连接在一起的。因此还需要屏蔽掉不相关的节点。文章是对Af与矩阵M进行哈德玛积，矩阵M可以认为是注意力里面的Mask，取得是一个节点与他最相邻的K个邻居，通过Top-K的方式选取。（更通俗来说，一个节点i分别和各个节点计算相关性，最相关的K个节点标记1，否则标记0.）<br> <img src="/2025/04/28/STIDGCN%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-5.png" alt="节点相关性的定义"><br>得到邻接矩阵就可以进行图卷积操作了，文章采用扩散GCN进行动态图卷积，扩散图卷积把节点的动态变化描述成一个“扩散”过程，扩散图卷积聚合了图中节点之间的信息。扩散信号院子目标节点和当前最近的节点。扩散GCN可以描述为：<br> <img src="/2025/04/28/STIDGCN%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-6.png" alt="扩散GCN"><br>这里的Hg就是最开始的TSConv表征后的隐向量，尺寸是C×N×t’，W 是自学习权重的矩阵，尺寸是N×N，Af是刚刚得到的邻接矩阵，大小N×N。这个地方矩阵乘的维度没有写的很清楚，纳闷了很久维度不一样怎么乘，看代码，我们可以看到矩阵相乘是在N维度上相乘，最终输出还是B×C×N×T（TODO 单独实验一下DGCN模块的输入、输出，验证）。<br> <img src="/2025/04/28/STIDGCN%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-7.png" alt="截图来自：https:&#x2F;&#x2F;github.com&#x2F;LiuAoyu1998&#x2F;STIDGCN&#x2F;blob&#x2F;main&#x2F;model.py"></p><p><strong>时空交互学习</strong><br>最终整个交互学习的过程，用公式的话可以描述成，和首图是对得上的：<br> <img src="/2025/04/28/STIDGCN%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-8.png" alt="交互学习过程"></p><h3 id="3-2-Decoder"><a href="#3-2-Decoder" class="headerlink" title="3.2 Decoder"></a>3.2 Decoder</h3><p>解码器用于输入编码器的编码后的特征He进行解码，得到最终的计算结果Y。首先隐特征He送入GLU门控线性单元，门控单元就是将Hg分别经过两层FC，其中一层加上sigmoid激活函数，并进行哈德玛积。经过激活和FC后得到最终的预测结果Y。文章指出：没有采用自回归的方式生成y设结果是为了提高计算效率、缩小误差积累。</p><p><img src="/2025/04/28/STIDGCN%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-9.png" alt="解码器结构"></p><h2 id="4-实验"><a href="#4-实验" class="headerlink" title="4. 实验"></a>4. 实验</h2><h3 id="4-1-对比实验"><a href="#4-1-对比实验" class="headerlink" title="4.1 对比实验"></a>4.1 对比实验</h3><p>模型采用了PEMS多个数据集，在多个数据集上进行测试，取得了SOTA的效果。</p><p><img src="/2025/04/28/STIDGCN%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-10.png" alt="评测结果"></p><p>评价指标选的是MAE,MAPE,RMSE</p><h3 id="4-2-消融实验"><a href="#4-2-消融实验" class="headerlink" title="4.2 消融实验"></a>4.2 消融实验</h3><p>消融实验：进行了四组消融实验：<br>w&#x2F;o IL：STIDGCN替换掉时空交互学习模块，使用串行策略进行学习。<br>w&#x2F;o TSConv: 移除TSConv模块<br>w&#x2F;o GG：移除图生成模块<br>w&#x2F;o DGCN： 使用普通GCN替换DGCN，并且输入的邻接矩阵采用预定义图</p>]]></content>
    
    
    <categories>
      
      <category>时空图</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>多模态表征学习 - 学习笔记</title>
    <link href="/2025/04/23/%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0/"/>
    <url>/2025/04/23/%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h1 id="1-表征学习的定义、分类和发展趋势"><a href="#1-表征学习的定义、分类和发展趋势" class="headerlink" title="1.表征学习的定义、分类和发展趋势"></a>1.表征学习的定义、分类和发展趋势</h1><h2 id="1-1-表征学习的定义"><a href="#1-1-表征学习的定义" class="headerlink" title="1.1 表征学习的定义"></a>1.1 表征学习的定义</h2><p><strong>表征学习的定义</strong>：表征学习（Representation Learning）是一种通过算法从数据中自动学习到有用特征的技术，其目的是将复杂的、高维的原始数据转化为机器学习能够高效处理的低维特征表示。表征学习对应的是经典机器学习中的“特征提取”模块，过往常常通过人工去提取特征，表征学习则将此过程自动化，通过机器学习算法处理。<br><strong>表征学习的模型输入</strong>：原始数据，其中包含高维度特征，例如：图像、文字、音频、视频、图等结构化或非结构化的数据<br><strong>表征学习输出</strong>：经过表征学习之后，提取出能被下游任务使用的低维特征。这里的特征可以是显式的也可以是隐式的。目前主流的技术路线是将表征层作为上游任务预训练，学习完成之后向下游任务传递隐式信息。模型的下游任务可以是分类、预测、生成式任务。</p><p><img src="/2025/04/23/%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%A1%A8%E5%BE%81.png" alt="多模态表征学习实例之Gemini模型架构"></p><h2 id="1-2-表征学习的分类"><a href="#1-2-表征学习的分类" class="headerlink" title="1.2 表征学习的分类"></a>1.2 表征学习的分类</h2><p>表征学习按照任务划分，可以划分为：</p><ul><li><p>文字表征</p><ul><li>核心：对文本进行表征，即自然语言符号信息表示成数字信息，方便下游任务处理</li><li>经典方法<ul><li>Word2Vec（Word Embedding）</li><li>Bert（Deep Model）</li></ul></li></ul></li><li><p>视觉表征</p><ul><li>核心：理解各种视觉图像数据（如照片、医学图像、文件扫描、视频流）等的语义</li><li>经典方法<ul><li>Vgg-16、 ResNet（CNN系）</li><li>MAE、VIT（Transformer系）</li></ul></li></ul></li><li><p>音频表征</p><ul><li>核心：从音频信号中提取对应的声音特征</li><li>经典方法<ul><li>Wav2Vec</li><li>SimCLR</li><li>MAE</li></ul></li></ul></li><li><p>图表征</p><ul><li>核心：将图数据映射到向量空间，以保留图的结构特征和语义特征</li><li>经典方法<ul><li>GCN</li><li>GAT</li></ul></li></ul></li><li><p>多模态表征</p><ul><li>核心：旨在融合多种数据模态（如：文本、图像、音频、视频等）来提高模型的感知与理解能力，实现跨模态信息的交互与融合</li><li>经典方法<ul><li>预训练：BEIT、MAE</li><li>语义对齐：BLIP、CLIP、BEIT等</li><li>大模型：GPT、Gemini等</li></ul></li></ul></li></ul><h2 id="1-3-表征学习的发展趋势"><a href="#1-3-表征学习的发展趋势" class="headerlink" title="1.3 表征学习的发展趋势"></a>1.3 表征学习的发展趋势</h2><h3 id="2018年以前：以CNN-RNN-DNN架构为主"><a href="#2018年以前：以CNN-RNN-DNN架构为主" class="headerlink" title="2018年以前：以CNN&#x2F;RNN&#x2F;DNN架构为主"></a>2018年以前：以CNN&#x2F;RNN&#x2F;DNN架构为主</h3><ul><li>AlexNet  【图像】</li><li>Vgg16  【图像】</li><li>ResNet  【图像】</li><li>Word2Vec 【文字】</li><li>GCN 【图】</li></ul><h3 id="2018-2023：-Transformer架构逐渐成为主流框架、"><a href="#2018-2023：-Transformer架构逐渐成为主流框架、" class="headerlink" title="2018-2023： Transformer架构逐渐成为主流框架、"></a>2018-2023： Transformer架构逐渐成为主流框架、</h3><ul><li>BERT 【文】</li><li>ViT 【图像】</li><li>GAT&#x2F;Graph Transformer 【图】</li><li>MAE 【图像&#x2F;视频】</li><li>CLIP 【图像】</li><li>VILT 【图像】</li></ul><h3 id="2023后：-与大模型架构相结合（所列出的模型全部属于多模态大模型）"><a href="#2023后：-与大模型架构相结合（所列出的模型全部属于多模态大模型）" class="headerlink" title="2023后： 与大模型架构相结合（所列出的模型全部属于多模态大模型）"></a>2023后： 与大模型架构相结合（所列出的模型全部属于多模态大模型）</h3><ul><li>Grok3</li><li>Qwen-VL</li><li>GPT-4o</li><li>Gemini</li><li>Deepseek-VL</li><li>LLAVA-NeXT</li><li>Mini-GPT</li><li>Doubao - 1.5</li><li>Cosmos</li></ul><h3 id="技术发展趋势："><a href="#技术发展趋势：" class="headerlink" title="技术发展趋势："></a>技术发展趋势：</h3><ul><li>小模型专注于计算机视觉、文字处理等机器学习经典领域，聚焦人脸识别、目标检测等专业任务</li><li>大模型时代多模态表征是标配，聚焦多模态理解（语义对齐、协同学习）与多模态生成方向</li><li>图像+文字+视频的多模态融合是主要研究方向，主要需要解决语义对齐、语义融合与协同学习的问题。</li></ul><h1 id="2-表征学习的主流架构"><a href="#2-表征学习的主流架构" class="headerlink" title="2. 表征学习的主流架构"></a>2. 表征学习的主流架构</h1><h2 id="2-1-图像表征经典架构———CNN结构"><a href="#2-1-图像表征经典架构———CNN结构" class="headerlink" title="2.1 图像表征经典架构———CNN结构"></a>2.1 图像表征经典架构———CNN结构</h2><p>说到表征就不得不说到CNN，关于CNN的结构就不多介绍了。我们在此思考，为什么CNN模型在CV领域取得了巨大的成功？<br>主要有以下几点原因：</p><ul><li><strong>感知性</strong>： 卷积层通过卷积操作和参数共享，能够提取图像的局部特征</li><li><strong>参数共享</strong>：相同的卷积核在不同的位置对图像进行卷积操作，共享参数减少了模型的复杂度，也增强了模型的泛化能力</li><li><strong>空间不变性</strong>：卷积操作具有平移不变性，即无论图像中的物体在图像中的位置如何变化，比如上下左右平移，最终都能被卷积核扫到，能够提取到相应的特征。</li></ul><p>例如 可以到<a href="https://poloclub.github.io/cnn-explainer/">卷积可视化网站</a>查看卷积操作都对图片做了什么。可以看到，浅层卷积分辨率高，提取点、颜色等基础特征，随着卷积的深入，逐渐提取到线段、边缘、轮廓、角点等特征，进行多层卷积之后的图像具有分辨率低的特点，从而能够和最终分类的抽象特征强相关联。这就是CNN模型起效的原因。<br><img src="/2025/04/23/%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0/image1.gif" alt="卷积操作可视化"></p><h2 id="2-2-图像表征改进架构———Transformer结构"><a href="#2-2-图像表征改进架构———Transformer结构" class="headerlink" title="2.2 图像表征改进架构———Transformer结构"></a>2.2 图像表征改进架构———Transformer结构</h2><h2 id="2-3-Transformer架构讨论分析"><a href="#2-3-Transformer架构讨论分析" class="headerlink" title="2.3 Transformer架构讨论分析"></a>2.3 Transformer架构讨论分析</h2><h1 id="3-多模态表征与多模态大模型"><a href="#3-多模态表征与多模态大模型" class="headerlink" title="3. 多模态表征与多模态大模型"></a>3. 多模态表征与多模态大模型</h1><h2 id="3-1-多模态表征的定义、动机和核心问题"><a href="#3-1-多模态表征的定义、动机和核心问题" class="headerlink" title="3.1 多模态表征的定义、动机和核心问题"></a>3.1 多模态表征的定义、动机和核心问题</h2><h2 id="3-2-多模态表征的经典结构"><a href="#3-2-多模态表征的经典结构" class="headerlink" title="3.2 多模态表征的经典结构"></a>3.2 多模态表征的经典结构</h2><h3 id="3-2-1-单塔结构（以VILT为例）"><a href="#3-2-1-单塔结构（以VILT为例）" class="headerlink" title="3.2.1 单塔结构（以VILT为例）"></a>3.2.1 单塔结构（以VILT为例）</h3><h3 id="3-2-2-双塔结构（以CLIP为例）"><a href="#3-2-2-双塔结构（以CLIP为例）" class="headerlink" title="3.2.2 双塔结构（以CLIP为例）"></a>3.2.2 双塔结构（以CLIP为例）</h3><h2 id="3-3-多模态大模型"><a href="#3-3-多模态大模型" class="headerlink" title="3.3 多模态大模型"></a>3.3 多模态大模型</h2><h3 id="3-3-1-多模态大模型的通用结构"><a href="#3-3-1-多模态大模型的通用结构" class="headerlink" title="3.3.1 多模态大模型的通用结构"></a>3.3.1 多模态大模型的通用结构</h3><h1 id="4-参考文献"><a href="#4-参考文献" class="headerlink" title="4. 参考文献"></a>4. 参考文献</h1>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch快速入门笔记</title>
    <link href="/2025/04/02/Pytorch%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/"/>
    <url>/2025/04/02/Pytorch%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/</url>
    
    <content type="html"><![CDATA[<p>本笔记从<a href="https://www.bilibili.com/video/BV1hE411t7RN/">小土堆Pytorch教程</a>中记录一些实用的Pytorch相关操作.</p><h1 id="1-加载数据"><a href="#1-加载数据" class="headerlink" title="1. 加载数据"></a>1. 加载数据</h1><h2 id="1-1-PIL"><a href="#1-1-PIL" class="headerlink" title="1.1 PIL"></a>1.1 PIL</h2><p>PIL类可以用于加载图像、保存图像等操作</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br>img = Image.<span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;data/hymenoptera_data/train/ants/342438950_a3da61deab.jpg&#x27;</span>)<br></code></pre></td></tr></table></figure><h2 id="1-2-DataSet"><a href="#1-2-DataSet" class="headerlink" title="1.2 DataSet"></a>1.2 DataSet</h2><p>DataSet是一个抽象类，需要实现其中的<code>__getitem__</code>方法，以及最好是实现<code>__len__</code>方法，不然不能用迭代器，用for循环的方式取数据,<br>以下是一个自定义数据集的设置方式，可以看到需要重写<code>__getitem__</code>方法取数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyDataSet</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, data_dir, transforms=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.data_dir = data_dir<br>        <span class="hljs-variable language_">self</span>.image_paths = os.listdir(data_dir)<br>        <span class="hljs-variable language_">self</span>.transforms = transforms<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, index</span>) -&gt; T_co:<br>        img_file_name = <span class="hljs-variable language_">self</span>.image_paths[index]<br>        img = Image.<span class="hljs-built_in">open</span>(<span class="hljs-variable language_">self</span>.data_dir + img_file_name)<br>        img_trans = <span class="hljs-variable language_">self</span>.transforms(img)<br>        <span class="hljs-keyword">return</span> img_trans<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.image_paths)<br></code></pre></td></tr></table></figure><h2 id="1-3-DataLoader"><a href="#1-3-DataLoader" class="headerlink" title="1.3 DataLoader"></a>1.3 DataLoader</h2><p>如果把DataSet看做一副牌，那么DataLoader就是用于定义如何发牌，或者对牌进行一些操作（洗牌、转换格式等），如果已经有一个数据集，那么可以通过这种方式定义data_loader</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">my_ds = MyDataSet(data_dir=<span class="hljs-string">&#x27;data/hymenoptera_data/val/ants/&#x27;</span>, transforms=trans)<br><span class="hljs-comment"># drop_last: 总长度除bs 除不尽的时候是否去掉最后一个</span><br><span class="hljs-comment"># batch_size: 批的量</span><br>data_loader = DataLoader(dataset=my_ds, batch_size=<span class="hljs-number">2</span>, shuffle=<span class="hljs-literal">True</span>, drop_last=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p>定义好的数据集，可以通过DataLoader加载，并通过for循环取数据，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">step = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> images <span class="hljs-keyword">in</span> data_loader:<br>    <span class="hljs-built_in">print</span>(images.shape)<br>    writer.add_images(<span class="hljs-string">&#x27;image_batch&#x27;</span>, images, step)  <span class="hljs-comment"># (tag,Image,step(不添加这个参数 tensorboard里面的step始终为零))</span><br>    step += <span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><h2 id="1-4-torchvision数据集的下载和使用"><a href="#1-4-torchvision数据集的下载和使用" class="headerlink" title="1.4 torchvision数据集的下载和使用"></a>1.4 torchvision数据集的下载和使用</h2><p>如果是一些成熟的数据集，比如CIFAR10，可以用封装好的方式获取数据集，这些数据集也是重写了DataSet类，可以传入transform</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">train_data = torchvision.datasets.CIFAR10(root=<span class="hljs-string">&#x27;../data&#x27;</span>, train=<span class="hljs-literal">True</span>, transform=torchvision.transforms.ToTensor(),<br>                                          download=<span class="hljs-literal">True</span>)<br>test_data = torchvision.datasets.CIFAR10(root=<span class="hljs-string">&#x27;../data&#x27;</span>, train=<span class="hljs-literal">False</span>, transform=torchvision.transforms.ToTensor(),<br>                                         download=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><h1 id="2-Tensorboard-的使用"><a href="#2-Tensorboard-的使用" class="headerlink" title="2. Tensorboard 的使用"></a>2. Tensorboard 的使用</h1><p>TensorBoard是一个可视化工具，它可以用来展示网络图、张量的指标变化、张量的分布情况等。特别是在训练网络的时候，我们可以设置不同的参数（比如：权重W、偏置B、卷积层数、全连接层数等），使用TensorBoader可以很直观的帮我们进行参数的选择。它通过运行一个本地服务器，来监听6006端口。在浏览器发出请求时，分析训练时记录的数据，绘制训练过程中的图像。</p><p>首先定义一个SummaryWriter()，然后就可以用writer里面的方法往tensorboard里面写数据，不仅可以添加过程量还可以添加单张图像。默认的路径保存到本地runs目录下，可以用SummaryWriter()</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">writer = SummaryWriter()<br><span class="hljs-comment"># 添加过程量（标量）</span><br><span class="hljs-keyword">for</span> n_iter <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    writer.add_scalar(<span class="hljs-string">&#x27;Loss/train&#x27;</span>, np.random.random(), n_iter)<br>    writer.add_scalar(<span class="hljs-string">&#x27;Loss/test&#x27;</span>, np.random.random(), n_iter)<br>    writer.add_scalar(<span class="hljs-string">&#x27;Accuracy/train&#x27;</span>, np.random.random(), n_iter)<br>    writer.add_scalar(<span class="hljs-string">&#x27;Accuracy/test&#x27;</span>, np.random.random(), n_iter)<br>writer.add_image(tag=<span class="hljs-string">&#x27;test&#x27;</span>, img_tensor=img_tensor)<br></code></pre></td></tr></table></figure><p>查看数据：cd到保存文件的文件夹下，输入<code>tensorboard --logdir runs</code> runs对应文件保存的目录，然后就可以通过访问<code>http://localhost:6006/#timeseries</code>查看记录的结果</p><h1 id="3-Transforms-的使用"><a href="#3-Transforms-的使用" class="headerlink" title="3. Transforms 的使用"></a>3. Transforms 的使用</h1><p>Transforms用来对一张图片进行一系列的转换，可以用Compose定义需要转换的内容</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">trans = transforms.Compose([<br>    transforms.ToTensor(),<br>    <span class="hljs-comment"># transforms.RandomCrop(size=(50,50)) 随机裁剪</span><br>    transforms.Resize((<span class="hljs-number">100</span>, <span class="hljs-number">100</span>))<br>])<br><br></code></pre></td></tr></table></figure><p>定义好转换之后，可以对单张图片进行转换，把图像传入就可以，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 添加图像</span><br>img = Image.<span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;data/hymenoptera_data/train/ants/342438950_a3da61deab.jpg&#x27;</span>)<br><span class="hljs-comment"># 图像转换</span><br>trans = transforms.Compose([<br>    transforms.ToTensor(),<br>    <span class="hljs-comment"># transforms.RandomCrop(size=(50,50)) # 随机裁剪</span><br>])<br>img_tensor = trans(img)<br></code></pre></td></tr></table></figure><h1 id="4-常用函数"><a href="#4-常用函数" class="headerlink" title="4. 常用函数"></a>4. 常用函数</h1><h2 id="4-1卷积函数"><a href="#4-1卷积函数" class="headerlink" title="4.1卷积函数"></a>4.1卷积函数</h2><p>卷积函数的定义网上有很多了就不再赘述了，定义一个卷积核，然后和现在的矩阵进行卷积操作，可得到一个结果。</p><p>借用知乎<a href="https://zhuanlan.zhihu.com/p/161660908">2D卷积,nn.Conv2d和F.conv2d</a>一段话：卷积操作：卷积核和扫过的小区域对应位置相乘再求和的操作，卷积完成后一般要加个偏置bias。一种Kernel如果分成多个通道上的子Kernel做卷积运算，最后运算结果还要加在一起后，再加偏置。</p><p>使用卷积运算的时候需要注意输入输出的尺寸，需要对齐，比如Conv2D 如果是函数就要求B ,C 两个维度要对齐。<br>需要注意的点是输入输出维度会根据stride、padding的设置改变，比如64×64的图像进去，不设置padding出来的图像可能就变成62×62了，如果还要保持图像尺寸一致（特别是复现论文的场景），需要反算一下stride和padding的值，这里公式在<a href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d">Pytorch Conv2d文档</a>，需要的时候直接查阅就好。</p><p>关于可视化展示卷积函数中的stride、padding、dilation参数的含义，可参考文档：<a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">Convolution arithmetic</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br>w = torch.rand(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)  <span class="hljs-comment"># 对应 B C ×卷积核的 H W，前2个维度对应上（在H W两个维度上进行卷积）</span><br>res = F.conv2d(<span class="hljs-built_in">input</span>=a, weight=w, stride=<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(res)<br><br>tensor([[[[-<span class="hljs-number">0.9163</span>, -<span class="hljs-number">1.4657</span>, -<span class="hljs-number">3.6013</span>,  ...,  <span class="hljs-number">0.1913</span>, -<span class="hljs-number">1.4308</span>, -<span class="hljs-number">1.1725</span>],<br>          [-<span class="hljs-number">1.7863</span>, -<span class="hljs-number">1.1487</span>, -<span class="hljs-number">3.5197</span>,  ..., -<span class="hljs-number">0.5010</span>, -<span class="hljs-number">2.3962</span>, -<span class="hljs-number">3.8177</span>],<br>          [-<span class="hljs-number">0.0863</span>, -<span class="hljs-number">0.3723</span>, -<span class="hljs-number">1.7177</span>,  ..., -<span class="hljs-number">1.9196</span>, -<span class="hljs-number">1.4938</span>, -<span class="hljs-number">2.6761</span>],<br>          ...,<br>          [ <span class="hljs-number">0.8136</span>, -<span class="hljs-number">4.5267</span>, -<span class="hljs-number">0.6807</span>,  ..., -<span class="hljs-number">2.2519</span>,  <span class="hljs-number">1.4239</span>, -<span class="hljs-number">0.9793</span>],<br>          [ <span class="hljs-number">1.8353</span>, -<span class="hljs-number">1.8440</span>, -<span class="hljs-number">3.9382</span>,  ..., -<span class="hljs-number">1.8193</span>,  <span class="hljs-number">2.7279</span>,  <span class="hljs-number">4.4726</span>],<br>          [ <span class="hljs-number">0.5444</span>,  <span class="hljs-number">1.2673</span>, -<span class="hljs-number">3.4205</span>,  ..., -<span class="hljs-number">2.3179</span>, -<span class="hljs-number">2.5870</span>, -<span class="hljs-number">1.7544</span>]]]])<br></code></pre></td></tr></table></figure><h2 id="4-2-池化函数"><a href="#4-2-池化函数" class="headerlink" title="4.2 池化函数"></a>4.2 池化函数</h2><p>池化函数是深度学习中常用的技术，主要用于降低数据的维度和减少计算量。常见的池化函数包括：</p><ol><li>最大池化（Max Pooling）：在池化窗口内选取最大值作为输出，能够提取图像中的主要特征。</li><li>平均池化（Average Pooling）：在池化窗口内取平均值作为输出，可以平滑输入数据，减少噪声的影响。</li><li>自适应池化（Adaptive Pooling）：根据输入的大小自动调整池化窗口的大小，以适应不同的输入尺寸。</li></ol><p>池化操作可以分为一维池化、二维池化和三维池化，具体取决于被池化的张量维数。池化不仅可以减小数据大小，还可以增加数据大小，具体取决于应用场景。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">a = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">64</span>, <span class="hljs-number">64</span>)<br>res = F.max_pool2d(a,kernel_size=<span class="hljs-number">2</span>) <span class="hljs-comment"># kernel_size指定2的话是默认2×2的2d（正方形）。而且池化默认区域不重叠的，默认步长就是kernel_size=2，这一点和卷积运算不一样</span><br><span class="hljs-built_in">print</span>(res.shape) <span class="hljs-comment"># torch.Size([1, 3, 32, 32])</span><br></code></pre></td></tr></table></figure><h1 id="5-神经网络的搭建"><a href="#5-神经网络的搭建" class="headerlink" title="5. 神经网络的搭建"></a>5. 神经网络的搭建</h1><h2 id="5-1-卷积层、池化层、非线性激活层"><a href="#5-1-卷积层、池化层、非线性激活层" class="headerlink" title="5.1 卷积层、池化层、非线性激活层"></a>5.1 卷积层、池化层、非线性激活层</h2><p>通过引入<code>torch.nn</code>引入常见神经网络的层，包括卷积层、池化层等.以及非线性激活层，RELU SOFTMAX之类的，具体就不再展开了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyNeuralNetwork</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, *args, **kwargs</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-built_in">super</span>().__init__(*args, **kwargs)<br>        <span class="hljs-variable language_">self</span>.conv1 = nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">64</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        <span class="hljs-variable language_">self</span>.conv2 = nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">64</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>)<br>        <span class="hljs-variable language_">self</span>.max_pool = nn.MaxPool2d(<span class="hljs-number">4</span>)<br>        <span class="hljs-variable language_">self</span>.relu = nn.ReLU()<br>        <span class="hljs-variable language_">self</span>.softmax = nn.Softmax()<br>        <span class="hljs-comment"># in_channels: int,</span><br>        <span class="hljs-comment"># out_channels: int,</span><br>        <span class="hljs-comment"># kernel_size: _size_2_t,</span><br>        <span class="hljs-comment"># stride: _size_2_t = 1,</span><br>        <span class="hljs-comment"># padding: Union[str, _size_2_t] = 0,</span><br>        <span class="hljs-comment"># dilation: _size_2_t = 1,</span><br><br>a = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">64</span>, <span class="hljs-number">64</span>)<br><br>mnn = MyNeuralNetwork()<br><br>res_conv1 = mnn.conv1(a)<br><span class="hljs-built_in">print</span>(res_conv1.shape)  <span class="hljs-comment"># torch.Size([1, 64, 64, 64])</span><br><br>res_conv2 = mnn.conv2(a)<br><span class="hljs-built_in">print</span>(res_conv2.shape)  <span class="hljs-comment"># torch.Size([1, 64, 62, 62])</span><br><br>res_max_pool = mnn.max_pool(a)<br><span class="hljs-built_in">print</span>(res_max_pool.shape)  <span class="hljs-comment"># torch.Size([1, 3, 16, 16])</span><br><br><br></code></pre></td></tr></table></figure><h2 id="5-2线性层及其他层"><a href="#5-2线性层及其他层" class="headerlink" title="5.2线性层及其他层"></a>5.2线性层及其他层</h2><ol><li>线性层：线性层又叫全连接层，其中每个神经元和上一层所有的神经元相连，使用<code>nn.Linear(in_features,out_features,bias)</code>定义,运算公式是 $$y&#x3D;xA^T+b$$ ，注意默认是加上bias的，即<code>bias=True</code><br>代码例子：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyNeuralNetwork</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, *args, **kwargs</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-variable language_">self</span>.linear = nn.Linear(<span class="hljs-number">64</span>,<span class="hljs-number">32</span>)<br>a = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">64</span>, <span class="hljs-number">64</span>)<br>res_l = mnn.linear(a)<br><span class="hljs-built_in">print</span>(res_l.shape) <span class="hljs-comment"># torch.Size([1, 3, 64, 32])</span><br></code></pre></td></tr></table></figure><ol start="2"><li>展平层：将多维度的张量展平。默认参数：<code>start_dim: int = 1, end_dim: int = -1</code> 从开始的维度展开到结束的维度<br>代码例子：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyNeuralNetwork</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, *args, **kwargs</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-variable language_">self</span>.flatten1 = nn.Flatten(start_dim=<span class="hljs-number">0</span>)<br>        <span class="hljs-variable language_">self</span>.flatten2 = nn.Flatten()<br>a = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">64</span>, <span class="hljs-number">64</span>)<br>res_f = mnn.flatten1(a)<br>res_f2 = mnn.flatten2(a)<br><span class="hljs-built_in">print</span>(res_f.shape) <span class="hljs-comment"># torch.Size([12288])</span><br><span class="hljs-built_in">print</span>(res_f2.shape) <span class="hljs-comment"># torch.Size([1, 12288])</span><br></code></pre></td></tr></table></figure><p>一般说来，Flatten层常用于把多维的输入一维化，常用在从卷积层到全连接层的过渡。Flatten默认不影响batch的大小（start_dim &#x3D;1 ）。</p><h2 id="5-3-Sequential的使用"><a href="#5-3-Sequential的使用" class="headerlink" title="5.3 Sequential的使用"></a>5.3 Sequential的使用</h2><p>nn.Sequential() 可以作为容器，里面放入模型的各种层，在forward的时候将会贯序列执行各层，通常有2种定义方式</p><ul><li>定义方式1</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br>model = nn.Sequential(<br>                  nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">20</span>,<span class="hljs-number">5</span>),<br>                  nn.ReLU(),<br>                  nn.Conv2d(<span class="hljs-number">20</span>,<span class="hljs-number">64</span>,<span class="hljs-number">5</span>),<br>                  nn.ReLU()<br>                )<br> <br><span class="hljs-built_in">print</span>(model)<br><span class="hljs-built_in">print</span>(model[<span class="hljs-number">2</span>]) <span class="hljs-comment"># 通过索引获取第几个层</span><br><span class="hljs-string">&#x27;&#x27;&#x27;运行结果为：</span><br><span class="hljs-string">Sequential(</span><br><span class="hljs-string">  (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))</span><br><span class="hljs-string">  (1): ReLU()</span><br><span class="hljs-string">  (2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))</span><br><span class="hljs-string">  (3): ReLU()</span><br><span class="hljs-string">)</span><br><span class="hljs-string">Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><ul><li>定义方式2：给每个层添加一个名称</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> OrderedDict<br>model = nn.Sequential(OrderedDict([<br>                  (<span class="hljs-string">&#x27;conv1&#x27;</span>, nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">20</span>,<span class="hljs-number">5</span>)),<br>                  (<span class="hljs-string">&#x27;relu1&#x27;</span>, nn.ReLU()),<br>                  (<span class="hljs-string">&#x27;conv2&#x27;</span>, nn.Conv2d(<span class="hljs-number">20</span>,<span class="hljs-number">64</span>,<span class="hljs-number">5</span>)),<br>                  (<span class="hljs-string">&#x27;relu2&#x27;</span>, nn.ReLU())<br>                ]))<br> <br><span class="hljs-built_in">print</span>(model)<br><span class="hljs-string">&#x27;&#x27;&#x27;运行结果为：</span><br><span class="hljs-string">Sequential(</span><br><span class="hljs-string">  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))</span><br><span class="hljs-string">  (relu1): ReLU()</span><br><span class="hljs-string">  (conv2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))</span><br><span class="hljs-string">  (relu2): ReLU()</span><br><span class="hljs-string">)</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><p>我们可以将前面所学的层组合起来，形成深层神经网络的架构，例如我们可以编写一个自己的网络如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyNeuralNetwork</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, *args, **kwargs</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-built_in">super</span>().__init__(*args, **kwargs)<br>        <span class="hljs-variable language_">self</span>.model = nn.Sequential(<br>            nn.Conv2d(<span class="hljs-number">3</span>,<span class="hljs-number">64</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),  <span class="hljs-comment"># 【1,64,64,64】 备注：ks=3，stride=1，padding = 1</span><br>            <span class="hljs-comment"># Hout(64) = (Hin(64) + 2×padding - dilation×[ks - 1] × 1 )/stride + 1</span><br>            nn.ReLU(),<br>            nn.Conv2d(<span class="hljs-number">64</span>,<span class="hljs-number">32</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),  <span class="hljs-comment"># 1,32,64,64</span><br>            nn.ReLU(),<br>            nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">16</span>, <span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>), <span class="hljs-comment"># 1,16,64,64</span><br>            nn.ReLU(),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>), <span class="hljs-comment"># 1,16,32,32</span><br>            nn.ReLU(),<br>            nn.Linear(<span class="hljs-number">32</span>,<span class="hljs-number">1024</span>), <span class="hljs-comment"># 1,16,32,1024</span><br>            nn.ReLU(),<br>            nn.Linear(<span class="hljs-number">1024</span>, <span class="hljs-number">1024</span>),  <span class="hljs-comment"># 1,16,32,1024</span><br>            nn.ReLU(),<br>            nn.Flatten(), <span class="hljs-comment"># 1,524288</span><br>            nn.Linear(<span class="hljs-number">524288</span>,<span class="hljs-number">10</span>),  <span class="hljs-comment"># 1,10</span><br>            nn.Softmax(dim= -<span class="hljs-number">1</span>)<br>        )<br><br>mnn = MyNeuralNetwork()<br><br>a = torch.randn(<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">64</span>,<span class="hljs-number">64</span>)<br><br>res = mnn.model(a)<br><br><span class="hljs-built_in">print</span>(res.shape) <span class="hljs-comment"># torch.Size([1, 10])</span><br><span class="hljs-built_in">print</span>(res) <span class="hljs-comment">#tensor([[0.0991, 0.0982, 0.0997, 0.0996, 0.1009, 0.1030, 0.0988, 0.1027, 0.0991,0.0988]], grad_fn=&lt;SoftmaxBackward0&gt;)</span><br></code></pre></td></tr></table></figure><p>通过使用Sequential()的方式可以便捷的完成网络的定义，快速实现网络。</p><h2 id="5-4-小网络搭建实战"><a href="#5-4-小网络搭建实战" class="headerlink" title="5.4 小网络搭建实战"></a>5.4 小网络搭建实战</h2><p>以vgg16这个网络（图待补充）为例，搭建模型如下（暂未添加Relu层），其实和我们之前写的模型很像</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyNeuralNetwork</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, *args, **kwargs</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-built_in">super</span>().__init__(*args, **kwargs)<br>        <span class="hljs-variable language_">self</span>.model= nn.Sequential(<br>            nn.Conv2d(<span class="hljs-number">3</span>,<span class="hljs-number">32</span>,<span class="hljs-number">5</span>,padding=<span class="hljs-number">2</span>),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br>            nn.Conv2d(<span class="hljs-number">32</span>,<span class="hljs-number">32</span>,<span class="hljs-number">5</span>,padding=<span class="hljs-number">2</span>),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br>            nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, <span class="hljs-number">5</span>, padding=<span class="hljs-number">2</span>),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br>            nn.Flatten(),<br>            nn.Linear(<span class="hljs-number">4096</span>,<span class="hljs-number">64</span>),<br>            nn.Linear(<span class="hljs-number">64</span>, <span class="hljs-number">10</span>),<br>        )<br><br>a = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">64</span>, <span class="hljs-number">64</span>)<br>mnn = MyNeuralNetwork()<br>res = mnn.model(a)<br><span class="hljs-built_in">print</span>(res.shape) <span class="hljs-comment">#torch.Size([1, 10])</span><br></code></pre></td></tr></table></figure><h1 id="6-损失函数与反向传播"><a href="#6-损失函数与反向传播" class="headerlink" title="6 损失函数与反向传播"></a>6 损失函数与反向传播</h1><h2 id="6-1-损失函数"><a href="#6-1-损失函数" class="headerlink" title="6.1 损失函数"></a>6.1 损失函数</h2><p>损失函数（Loss Function）是一个衡量预测结果与真实结果之间差异的函数 ，也称为误差函数。它通过计算模型的预测值与真实值之间的不一致程度，来评估模型的性能.<br>根据任务不同，选择的损失函数也不同，对于回归任务，常见的损失函数有<code>MSELoss</code>,对于分类任务常见的损失函数有交叉熵损失<code>CrossEntropyLoss</code><br>交叉熵的损失函数可以描述为 </p><p><img src="/2025/04/02/Pytorch%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/image.png" alt="交叉熵的损失函数"></p><p>举例说明：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br>x = torch.tensor([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.3</span>]]) <span class="hljs-comment"># 预测三个类别概率分别是0.1,0.2,0.3</span><br>y = torch.tensor([<span class="hljs-number">1</span>]) <span class="hljs-comment"># 答案是1</span><br>loss = F.cross_entropy(x, y) <span class="hljs-comment"># 计算交叉熵 loss = -0.2 + ln(e^0.1+e^0.2+e^0.3) = 1.10194284823</span><br><span class="hljs-built_in">print</span>(loss) <span class="hljs-comment"># tensor(1.1019)</span><br></code></pre></td></tr></table></figure><p>其他的案例也差不多</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.tensor([[<span class="hljs-number">0.55</span>, <span class="hljs-number">0.88</span>]]) <span class="hljs-comment"># 预测值</span><br>y = torch.tensor([[<span class="hljs-number">0.5</span>, <span class="hljs-number">0.8</span>]]) <span class="hljs-comment"># 真实值</span><br>loss_l1 = F.l1_loss(x, y) <span class="hljs-comment"># L1Loss 一阶距</span><br>loss_mse = F.mse_loss(x, y) <span class="hljs-comment"># MSE_LOSS</span><br><span class="hljs-built_in">print</span>(loss_l1) <span class="hljs-comment"># tensor(0.0650)</span><br><span class="hljs-built_in">print</span>(loss_mse) <span class="hljs-comment"># tensor(0.0044)</span><br><br>loss_layer = nn.L1Loss(reduction=<span class="hljs-string">&#x27;sum&#x27;</span>)  <span class="hljs-comment"># 备注：reduction默认是mean，用mean的话结果是0.065</span><br>loss_l1_by_layer = loss_layer(x, y)<br><span class="hljs-built_in">print</span>(loss_l1_by_layer)  <span class="hljs-comment"># tensor(0.1300)</span><br></code></pre></td></tr></table></figure><h2 id="6-2-反向传播"><a href="#6-2-反向传播" class="headerlink" title="6.2 反向传播"></a>6.2 反向传播</h2><p>首先说一下什么是反向传播算法。<br>反向传播算法(Backpropagation，简称BP算法)是“误差反向传播”的简称，是适合于多层神经元网络的一种学习算法，它建立在梯度下降法的基础上。梯度下降法是训练神经网络的常用方法，许多的训练方法都是基于梯度下降法改良出来的，因此了解梯度下降法很重要。梯度下降法通过计算损失函数的梯度，并将这个梯度反馈给最优化函数来更新权重以最小化损失函数。</p><p>在PyTorch中，loss.backward()函数用于计算模型参数相对于损失函数的梯度。</p><p>前向传播</p><p>首先，模型通过前向传播计算输出值。在这个过程中，PyTorch会记录计算图（Computation Graph），这个计算图记录了从输入到输出的每一步运算及其依赖关系。每个张量（Tensor）都有一个.grad_fn属性，指向一个函数，这个函数描述了如何计算这个张量关于其输入的梯度。</p><p>反向传播</p><p>当调用loss.backward()时，PyTorch开始反向遍历计算图。这个过程从损失函数开始，沿着图反向传播误差，计算每一个参与运算的张量关于损失的梯度。这是通过链式法则（Chain Rule）完成的，即将损失对某个中间变量的导数分解为其后续操作导数的乘积。</p><p>梯度计算<br>在反向传播过程中，每个运算都会计算其输出关于输入的梯度，并将这个梯度累积到输入张量的.grad属性中（如果是标量损失，它没有.grad属性）。这意味着如果一个张量被多个路径使用，它的.grad属性会累积从所有路径来的梯度.</p><p>在使用loss.backward()时，有几个重要的注意事项：</p><pre><code class="hljs">梯度归零：在每次反向传播之前，通常需要调用optimizer.zero_grad()来将梯度归零，以避免梯度累加</code></pre><h2 id="6-3-优化器"><a href="#6-3-优化器" class="headerlink" title="6.3 优化器"></a>6.3 优化器</h2><p>优化器决定了模型以何种方式的梯度下降算法更新模型。常见的优化器有 SGD, adam等。</p><p>在PyTorch中，optimizer.step()是优化器对象的一个方法，用于执行模型参数的更新。在深度学习训练过程中，参数更新是通过反向传播算法计算损失函数的梯度后，使用优化器根据这些梯度进行的。optimizer.step()方法正是用于根据梯度和学习率等超参数来更新模型参数，从而使损失函数值最小化的步骤</p><h1 id="7-使用常用模型"><a href="#7-使用常用模型" class="headerlink" title="7 使用常用模型"></a>7 使用常用模型</h1><h2 id="7-1-使用库的方式调用常用模型"><a href="#7-1-使用库的方式调用常用模型" class="headerlink" title="7.1 使用库的方式调用常用模型"></a>7.1 使用库的方式调用常用模型</h2><p>一些常用的模型，比较经典的模型都是包装在库里面了，可以通过<code>torchvision.models.xxx</code>调用模型.<br>例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs python">vgg16 = torchvision.models.vgg16(pretrained = <span class="hljs-literal">False</span>)<br><br><span class="hljs-built_in">print</span>(vgg16)<br><br>输出：<br>VGG(<br>  (features): Sequential(<br>    (<span class="hljs-number">0</span>): Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">64</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>    (<span class="hljs-number">1</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">2</span>): Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>    (<span class="hljs-number">3</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">4</span>): MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">0</span>, dilation=<span class="hljs-number">1</span>, ceil_mode=<span class="hljs-literal">False</span>)<br>    (<span class="hljs-number">5</span>): Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>    (<span class="hljs-number">6</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">7</span>): Conv2d(<span class="hljs-number">128</span>, <span class="hljs-number">128</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>    (<span class="hljs-number">8</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">9</span>): MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">0</span>, dilation=<span class="hljs-number">1</span>, ceil_mode=<span class="hljs-literal">False</span>)<br>    (<span class="hljs-number">10</span>): Conv2d(<span class="hljs-number">128</span>, <span class="hljs-number">256</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>    (<span class="hljs-number">11</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">12</span>): Conv2d(<span class="hljs-number">256</span>, <span class="hljs-number">256</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>    (<span class="hljs-number">13</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">14</span>): Conv2d(<span class="hljs-number">256</span>, <span class="hljs-number">256</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>    (<span class="hljs-number">15</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">16</span>): MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">0</span>, dilation=<span class="hljs-number">1</span>, ceil_mode=<span class="hljs-literal">False</span>)<br>    (<span class="hljs-number">17</span>): Conv2d(<span class="hljs-number">256</span>, <span class="hljs-number">512</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>    (<span class="hljs-number">18</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">19</span>): Conv2d(<span class="hljs-number">512</span>, <span class="hljs-number">512</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>    (<span class="hljs-number">20</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">21</span>): Conv2d(<span class="hljs-number">512</span>, <span class="hljs-number">512</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>    (<span class="hljs-number">22</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">23</span>): MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">0</span>, dilation=<span class="hljs-number">1</span>, ceil_mode=<span class="hljs-literal">False</span>)<br>    (<span class="hljs-number">24</span>): Conv2d(<span class="hljs-number">512</span>, <span class="hljs-number">512</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>    (<span class="hljs-number">25</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">26</span>): Conv2d(<span class="hljs-number">512</span>, <span class="hljs-number">512</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>    (<span class="hljs-number">27</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">28</span>): Conv2d(<span class="hljs-number">512</span>, <span class="hljs-number">512</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>    (<span class="hljs-number">29</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">30</span>): MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">0</span>, dilation=<span class="hljs-number">1</span>, ceil_mode=<span class="hljs-literal">False</span>)<br>  )<br>  (avgpool): AdaptiveAvgPool2d(output_size=(<span class="hljs-number">7</span>, <span class="hljs-number">7</span>))<br>  (classifier): Sequential(<br>    (<span class="hljs-number">0</span>): Linear(in_features=<span class="hljs-number">25088</span>, out_features=<span class="hljs-number">4096</span>, bias=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">1</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">2</span>): Dropout(p=<span class="hljs-number">0.5</span>, inplace=<span class="hljs-literal">False</span>)<br>    (<span class="hljs-number">3</span>): Linear(in_features=<span class="hljs-number">4096</span>, out_features=<span class="hljs-number">4096</span>, bias=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">4</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">5</span>): Dropout(p=<span class="hljs-number">0.5</span>, inplace=<span class="hljs-literal">False</span>)<br>    (<span class="hljs-number">6</span>): Linear(in_features=<span class="hljs-number">4096</span>, out_features=<span class="hljs-number">1000</span>, bias=<span class="hljs-literal">True</span>)<br>  )<br>)<br><br></code></pre></td></tr></table></figure><h2 id="7-2-对常用模型进行增加或修改"><a href="#7-2-对常用模型进行增加或修改" class="headerlink" title="7.2 对常用模型进行增加或修改"></a>7.2 对常用模型进行增加或修改</h2><ol><li>增加某层</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">vgg16.features.add_module(<span class="hljs-string">&#x27;relu&#x27;</span>,torch.nn.ReLU())<br>vgg16.classifier.add_module(<span class="hljs-string">&#x27;linear&#x27;</span>,torch.nn.Linear(<span class="hljs-number">1000</span>,<span class="hljs-number">10</span>))<br>例如分类器加上之后，模型结构如下：<br>  (classifier): Sequential(<br>    (<span class="hljs-number">0</span>): Linear(in_features=<span class="hljs-number">25088</span>, out_features=<span class="hljs-number">4096</span>, bias=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">1</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">2</span>): Dropout(p=<span class="hljs-number">0.5</span>, inplace=<span class="hljs-literal">False</span>)<br>    (<span class="hljs-number">3</span>): Linear(in_features=<span class="hljs-number">4096</span>, out_features=<span class="hljs-number">4096</span>, bias=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">4</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">5</span>): Dropout(p=<span class="hljs-number">0.5</span>, inplace=<span class="hljs-literal">False</span>)<br>    (<span class="hljs-number">6</span>): Linear(in_features=<span class="hljs-number">4096</span>, out_features=<span class="hljs-number">1000</span>, bias=<span class="hljs-literal">True</span>)<br>    (linear): Linear(in_features=<span class="hljs-number">1000</span>, out_features=<span class="hljs-number">10</span>, bias=<span class="hljs-literal">True</span>)<br>  )<br></code></pre></td></tr></table></figure><ol start="2"><li>修改某层</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">vgg16.classifier[<span class="hljs-number">7</span>]=nn.Linear(<span class="hljs-number">1000</span>,<span class="hljs-number">20</span>) <span class="hljs-comment"># 7 是模型的第几层</span><br><br>上面的代码执行之后会对刚添加的线性层修改输出特征节点的个数，改成了<span class="hljs-number">55</span>个<br>  (classifier): Sequential(<br>    (<span class="hljs-number">0</span>): Linear(in_features=<span class="hljs-number">25088</span>, out_features=<span class="hljs-number">4096</span>, bias=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">1</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">2</span>): Dropout(p=<span class="hljs-number">0.5</span>, inplace=<span class="hljs-literal">False</span>)<br>    (<span class="hljs-number">3</span>): Linear(in_features=<span class="hljs-number">4096</span>, out_features=<span class="hljs-number">4096</span>, bias=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">4</span>): ReLU(inplace=<span class="hljs-literal">True</span>)<br>    (<span class="hljs-number">5</span>): Dropout(p=<span class="hljs-number">0.5</span>, inplace=<span class="hljs-literal">False</span>)<br>    (<span class="hljs-number">6</span>): Linear(in_features=<span class="hljs-number">4096</span>, out_features=<span class="hljs-number">1000</span>, bias=<span class="hljs-literal">True</span>)<br>    (linear): Linear(in_features=<span class="hljs-number">1000</span>, out_features=<span class="hljs-number">55</span>, bias=<span class="hljs-literal">True</span>)<br>  )<br></code></pre></td></tr></table></figure><ol start="3"><li>删除某层<br>如果想删除某一层,直接将其删除即可，命令为</li></ol><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-selector-tag">del</span> vgg16<span class="hljs-selector-class">.classifier</span><span class="hljs-selector-attr">[7]</span><br></code></pre></td></tr></table></figure><ol start="4"><li>冻结部分层<br>我们现在只想微调最后的fc1层，其他层的参数冻结不训练，可以用<code>requires_grad = True</code> 或 <code>False</code> 来控制是否参与梯度回传</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 冻结fc1层的参数</span><br><span class="hljs-keyword">for</span> name, param <span class="hljs-keyword">in</span> model.named_parameters():<br>    <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;fc1&quot;</span> <span class="hljs-keyword">in</span> name:<br>        param.requires_grad = <span class="hljs-literal">True</span><br>    <span class="hljs-keyword">else</span>:<br>        param.requires_grad = <span class="hljs-literal">False</span><br><br><span class="hljs-comment"># 在优化的过程中，只传入需要更新的参数的层给优化器（采用过滤器选出参与梯度回传的层）</span><br>optimizer = optim.SGD(<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> p: p.requires_grad, model.parameters()), lr=<span class="hljs-number">1e-2</span>)<br></code></pre></td></tr></table></figure><h1 id="8-完整的训练流程"><a href="#8-完整的训练流程" class="headerlink" title="8 完整的训练流程"></a>8 完整的训练流程</h1><p>包括数据集准备，dataLoader准备、网络构建、损失函数定义、循环、计算误差、tensorboard可视化等</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br><br>train_data = torchvision.datasets.CIFAR10(root=<span class="hljs-string">&#x27;/data&#x27;</span>, train=<span class="hljs-literal">True</span>, transform=torchvision.transforms.ToTensor(),<br>                                          download=<span class="hljs-literal">True</span>)<br>test_data = torchvision.datasets.CIFAR10(root=<span class="hljs-string">&#x27;/data&#x27;</span>, train=<span class="hljs-literal">False</span>, transform=torchvision.transforms.ToTensor(),<br>                                         download=<span class="hljs-literal">True</span>)<br><br>train_data_size = <span class="hljs-built_in">len</span>(train_data)<br>test_data_size = <span class="hljs-built_in">len</span>(test_data)<br><br>train_data_loader = DataLoader(train_data, batch_size=<span class="hljs-number">64</span>)<br>test_data_loader = DataLoader(test_data, batch_size=<span class="hljs-number">64</span>)<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyNeuralNetwork</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, *args, **kwargs</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-built_in">super</span>().__init__(*args, **kwargs)<br>        <span class="hljs-variable language_">self</span>.model = nn.Sequential(<br>            nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br>            nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br>            nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br>            nn.Flatten(),<br>            nn.Linear(<span class="hljs-number">64</span> * <span class="hljs-number">4</span> * <span class="hljs-number">4</span>, <span class="hljs-number">64</span>),<br>            nn.Linear(<span class="hljs-number">64</span>, <span class="hljs-number">10</span>)<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.model(x)<br><br><br>mnn = MyNeuralNetwork()<br><span class="hljs-keyword">if</span> torch.cuda.is_available():<br>    mnn = mnn.cuda()<br><br>loss_fn = nn.CrossEntropyLoss()<br>loss_fn = loss_fn.cuda()<br><br>lr = <span class="hljs-number">1e-2</span><br>optim = torch.optim.SGD(mnn.parameters(), lr=lr)<br><br>writer = SummaryWriter(<span class="hljs-string">&#x27;../logs_train&#x27;</span>)<br>epoch = <span class="hljs-number">10</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epoch):<br>    mnn.train()<br>    train_step = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> train_data_loader:<br>        imgs, targets = data<br>        imgs = imgs.cuda()<br>        targets = targets.cuda()<br>        outputs = mnn(imgs)<br>        loss = loss_fn(outputs, targets)<br><br>        optim.zero_grad()<br>        loss.backward()<br>        optim.step()<br><br>        train_step = train_step + <span class="hljs-number">1</span><br>        <span class="hljs-keyword">if</span> train_step % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;训练次数：&#123;&#125;，loss = &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(train_step, loss.item()))<br>            writer.add_scalar(<span class="hljs-string">&quot;train_loss&quot;</span>, loss.item(), train_step)<br><br>    mnn.<span class="hljs-built_in">eval</span>()<br>    total_test_loss = <span class="hljs-number">0</span><br>    total_accuracy = <span class="hljs-number">0</span><br>    total_test_step = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br><br>        <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> test_data_loader:<br>            imgs, targets = data<br>            imgs = imgs.cuda()<br>            targets = targets.cuda()<br>            outputs = mnn(imgs)<br>            loss = loss_fn(outputs, targets)<br>            total_test_loss += loss.item()<br>            <span class="hljs-comment"># 求正确率</span><br>            accuracy = (outputs.argmax(<span class="hljs-number">1</span>) == targets).<span class="hljs-built_in">sum</span>()<br>            total_accuracy += accuracy<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;整体测试集上的loss:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(total_test_loss))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;整体测试集上的正确率:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(total_accuracy / test_data_size))<br>    writer.add_scalar(<span class="hljs-string">&quot;test_loss&quot;</span>, total_test_loss, total_test_step)<br>    writer.add_scalar(<span class="hljs-string">&quot;test_accuracy&quot;</span>, total_accuracy / test_data_size, total_test_step)<br>    total_test_step = total_test_step + <span class="hljs-number">1</span><br><br>    <span class="hljs-comment"># 保存模型</span><br>    torch.save(mnn, <span class="hljs-string">&quot;mnn&#123;&#125;.pth&quot;</span>.<span class="hljs-built_in">format</span>(i))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;模型已保存&quot;</span>)<br><br>writer.close()<br><br></code></pre></td></tr></table></figure><p>运行代码的效果：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs text">训练次数：100，loss = 2.2861804962158203<br>训练次数：200，loss = 2.2651848793029785<br>训练次数：300，loss = 2.2193050384521484<br>训练次数：400，loss = 2.1087183952331543<br>训练次数：500，loss = 2.0523011684417725<br>训练次数：600，loss = 1.9955447912216187<br>训练次数：700，loss = 1.9990053176879883<br>整体测试集上的loss:319.1352970600128<br>整体测试集上的正确率:0.2669000029563904<br>模型已保存<br></code></pre></td></tr></table></figure><h1 id="8-1-使用GPU训练"><a href="#8-1-使用GPU训练" class="headerlink" title="8.1 使用GPU训练"></a>8.1 使用GPU训练</h1><p>除了8.1的方式在所有张量用<code>.cuda()</code>送入显存的方式使用GPU训练外，还可以用<code>tensor.to(device)</code>的方式送入显存</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">device = <span class="hljs-string">&#x27;cuda&#x27;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;cpu&#x27;</span><br><br>net = Net().to(device)<br>imgs = imgs.to(device)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Run&#39;s Studio 重新出发</title>
    <link href="/2025/03/31/hello-world/"/>
    <url>/2025/03/31/hello-world/</url>
    
    <content type="html"><![CDATA[<pre><code class="hljs">原先网站的source因为换电脑的缘故没有保存，只能重新开一份博客，记录工作中的心得体会~</code></pre>]]></content>
    
    
    <categories>
      
      <category>杂谈</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>安装和配置Pytorch和cuda</title>
    <link href="/2025/03/16/%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AEPytorch%E5%92%8Ccuda/"/>
    <url>/2025/03/16/%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AEPytorch%E5%92%8Ccuda/</url>
    
    <content type="html"><![CDATA[<h1 id="安装方法"><a href="#安装方法" class="headerlink" title="安装方法"></a>安装方法</h1><p>请参考：<a href="https://www.cnblogs.com/tryhardwy/p/14659131.html">https://www.cnblogs.com/tryhardwy/p/14659131.html</a></p><ol><li>卸载掉旧版本torch torchvision</li><li>先到（<a href="https://pytorch.org/get-started/locally/%EF%BC%89%E6%9F%A5%E5%88%B0%E7%A8%B3%E5%AE%9A%E7%89%88%E6%9C%ACtorch%E5%AF%B9%E5%BA%94%E7%9A%84cuda">https://pytorch.org/get-started/locally/）查到稳定版本torch对应的cuda</a></li><li>下载并安装cuda</li><li>到（<a href="https://pytorch.org/get-started/locally/%EF%BC%89%E6%8C%89%E7%85%A7%E5%AF%B9%E5%BA%94%E7%89%88%E6%9C%AC%E5%AE%89%E8%A3%85torch">https://pytorch.org/get-started/locally/）按照对应版本安装torch</a></li><li>安装完成后，进入python验证。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br>torch.__version__<br>torch.cuda.is_available()<br></code></pre></td></tr></table></figure><p>显示True则安装成功。</p><h1 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h1><p>注意：直接粘贴</p><p><code>pip install torch torchvision torchaudio -f https://download.pytorch.org/whl/torch_stable.html</code> ，安装的是cpu版！</p><p>带cuda版本正确的安装语句是：</p><p><code>pip3 install torch==2.6.0+cu118 torchvision==0.21.0+cu118 --index-url https://download.pytorch.org/whl/cu118</code></p><p>如果上述地址下载太慢，还可以换用国内源例如aliyun（备注：阿里云的torch版本不全，部分最新版本无法下载，经过实验2.1.0版本可以下载并安装）<br><code>pip install torch==2.1.0+cu118 --use-deprecated=legacy-resolver  --no-cache-dir -f https://mirrors.aliyun.com/pytorch-wheels/cu118</code></p><p>其他的包 如果国内地址下载太慢，可以用以下常用镜像：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs awk">常见镜像源<br><br>清华：https:<span class="hljs-regexp">//</span>pypi.tuna.tsinghua.edu.cn<span class="hljs-regexp">/simple/</span><br>阿里云：http:<span class="hljs-regexp">//mi</span>rrors.aliyun.com<span class="hljs-regexp">/pypi/</span>simple/<br>中国科技大学：https:<span class="hljs-regexp">//</span>pypi.mirrors.ustc.edu.cn<span class="hljs-regexp">/simple/</span><br>华中科技大学：http:<span class="hljs-regexp">//</span>pypi.hustunique.com<span class="hljs-regexp">/simple/</span><br>上海交通大学：https:<span class="hljs-regexp">//mi</span>rror.sjtu.edu.cn<span class="hljs-regexp">/pypi/</span>web<span class="hljs-regexp">/simple/</span><br></code></pre></td></tr></table></figure><p>如果使用不带https的连接，还需要加上<code>--trusted-host mirrors.xxx.com</code><br>例如<br><code>pip install numpy&lt;=2.0.0 -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com</code></p>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Hexo新建博客并上传Github全流程</title>
    <link href="/2025/03/04/hexo%E6%96%B0%E5%BB%BA%E6%93%8D%E4%BD%9C%E5%B9%B6%E4%B8%8A%E4%BC%A0%E6%B5%81%E7%A8%8B/"/>
    <url>/2025/03/04/hexo%E6%96%B0%E5%BB%BA%E6%93%8D%E4%BD%9C%E5%B9%B6%E4%B8%8A%E4%BC%A0%E6%B5%81%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="step-1-安装node-js和git环境"><a href="#step-1-安装node-js和git环境" class="headerlink" title="step 1. 安装node.js和git环境"></a>step 1. 安装node.js和git环境</h1><p>可参考教程<a href="https://blog.csdn.net/suwyer/article/details/125562473%60">node和git的安装以及环境配置 (Windows)</a></p><h1 id="step-2-安装hexo"><a href="#step-2-安装hexo" class="headerlink" title="step 2. 安装hexo"></a>step 2. 安装hexo</h1><p>此时已经安装好了node.js和git，下面开始hexo的安装<br>首先在某一磁盘目录下创建文件夹，例如F盘，创建文件夹名为 Blog<br>进入Blog文件夹，右键鼠标-&gt;选择<code>Git Bash Here</code><br>输入 <code>npm install -g hexo-cli</code> ，并耐心等待一段时间<br>输入 <code>npm install hexo -save</code>，也耐心等待一段时间<br>此时我们可以发现，在Blog文件夹中多了许多内容<br>在Blog文件夹中新建文件夹，命名为hexo<br>关闭当前Git Bash，进入hexo文件夹，右键鼠标-&gt;选择Git Bash Here，或者直接在当前的Git Bash中输入 cd hexo<br>输入hexo init，初始化hexo环境，耐心等待一段时间<br>输入npm install，安装npm依赖包，耐心等待一段时间<br>输入hexo generate或者是hexo g，生成静态页面，耐心等待一段时间<br>输入hexo server或者是hexo s，生成本地服务。我们每次写完博客后，可以先在本地预览一下看看有没有什么问题，然后再发布到网上。<br>接着，我们在浏览器中访问<a href="http://localhost:4000/%EF%BC%8C%E8%BF%99%E5%B0%B1%E6%98%AF%E5%9C%A8%E6%9C%AC%E5%9C%B0%E7%94%9F%E6%88%90%E7%9A%84%E4%B8%80%E4%B8%AA%E5%8D%9A%E5%AE%A2%E3%80%82">http://localhost:4000/，这就是在本地生成的一个博客。</a><br>在Git Bash中按下Ctrl+C，可以关闭当前端口服务。</p><p>到这里，我们的本地博客就已经搭建完成了。下面将介绍如何与Github连接，将博客上传Internet</p><h1 id="step-3-新建Github仓库"><a href="#step-3-新建Github仓库" class="headerlink" title="step 3. 新建Github仓库"></a>step 3. 新建Github仓库</h1><p>登录到自己的Github中，新建一个仓库，命名为username.github.io，其中的username是你的用户名，勾选Initialiaze this repository with a README，创建仓库<br>我们可以访问自己的<code>username.github.io</code><br>返回<code>username.github.io</code>的仓库中，复制Git地址</p><h1 id="step-4-本地操作"><a href="#step-4-本地操作" class="headerlink" title="step 4.本地操作"></a>step 4.本地操作</h1><p>我们在<code>/Blog/hexo/</code>文件夹中，找到<code>_config.yml</code>文件，用文本编辑器打开它<br>将最下面的deploy改为下图所示的内容，其中repo的地址就是刚才我们复制的Git地址，修改好后保存退出 【注】修改内容中的:和后面的字母之间要有一个空格，否则后续内容会报错<br>接下来，我们暂且不考虑新建文章，在<code>Git Bash</code>中执行<code>npm install hexo-deployer-git </code>–save命令，耐心等待一段时间<br>最后执行 <code>hexo deploy</code>或者<code>hexo d</code>【注】这一步需要保证Github上拥有本机的公钥，可以自行查找解决办法<br>最后，成功部署</p><h1 id="step-5-上传博客"><a href="#step-5-上传博客" class="headerlink" title="step 5.上传博客"></a>step 5.上传博客</h1><h2 id="新建文章"><a href="#新建文章" class="headerlink" title="新建文章"></a>新建文章</h2><p>在Git Bash中输入<code>hexo new title</code>，其中，title就是我们这篇文章的名字。我们可以看到，在\Blog\hexo\source_posts\ 文件夹中新建了一个名称为<code>Test1.md</code>的文件<br>我们去编辑一下这个文件，此处需要Linux的部分知识，可以自行上网查找<br>编辑结束后，保存退出</p><h2 id="上传"><a href="#上传" class="headerlink" title="上传"></a>上传</h2><p>使用hexo g，生成静态文件<br>使用hexo d来将文档部署到Github上<br>最后我们访问username.github.io，发现刚才编辑的文档已经成功发布到了Internet上面<br>————————————————</p><p>版权声明：本文为博主原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接和本声明。</p><p>原文链接：<a href="https://blog.csdn.net/qq_43669381/article/details/107823432">https://blog.csdn.net/qq_43669381/article/details/107823432</a></p><p>部署流程（带图版）：<a href="https://blog.csdn.net/clearloe/article/details/139879493">https://blog.csdn.net/clearloe/article/details/139879493</a></p><h2 id="乱码问题解决方法"><a href="#乱码问题解决方法" class="headerlink" title="乱码问题解决方法"></a>乱码问题解决方法</h2><p>需要注意：如果本地部署没有出现乱码，但是主题上传到github上可能出现乱码，排版不正常的情况，此时要修改 <code>__config.yaml</code> 文件,修改：<br>url: <a href="https://yourpage.github.io/">https://yourpage.github.io</a> （自己的主页的网址，最后不要有&#x2F;）<br>root: &#x2F;   （增加这一行）<br>保存后重新生成并部署即可正常显示。</p><h3 id="使用vscode更改粘贴图片默认位置"><a href="#使用vscode更改粘贴图片默认位置" class="headerlink" title="使用vscode更改粘贴图片默认位置"></a>使用vscode更改粘贴图片默认位置</h3><pre><code class="hljs">点击小齿轮，打开设置输入 markdown.copy, 找到 Markdown&gt; Copy Files:Destination新增项, Key为: **/*.md, value为目标路径：assets/$&#123;documentBaseName&#125;/$&#123;fileName&#125;</code></pre>]]></content>
    
    
    <categories>
      
      <category>常用知识</category>
      
    </categories>
    
    
  </entry>
  
  
  
  
</search>
